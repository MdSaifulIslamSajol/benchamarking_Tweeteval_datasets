learning rate  : 0.0001
epochs : 30
                                                text  label
0  @user nice new signage. Are you not concerned ...      0
1  A woman who you fucked multiple times saying y...      1
2  @user @user real talk do you have eyes or were...      1
3  your girlfriend lookin at me like a groupie in...      1
4                        Hysterical woman like @user      0
                                                text
0  @user nice new signage. Are you not concerned ...
1  A woman who you fucked multiple times saying y...
2  @user @user real talk do you have eyes or were...
3  your girlfriend lookin at me like a groupie in...
4                        Hysterical woman like @user
   label
0      0
1      1
2      1
3      1
4      0
                                                text  label
0  @user @user If book Claire wanted to "stay in ...      0
1  After arriving in the EU refugees make protest...      0
2                                                 ðŸ˜³ðŸ‘‡      0
3  @user Worst thing is if they are that stupid t...      1
4  @user Say's the HYSTERICAL woman. It is woman ...      0
                                                text
0  @user @user If book Claire wanted to "stay in ...
1  After arriving in the EU refugees make protest...
2                                                 ðŸ˜³ðŸ‘‡
3  @user Worst thing is if they are that stupid t...
4  @user Say's the HYSTERICAL woman. It is woman ...
   label
0      0
1      0
2      0
3      1
4      0
                                                text  label
0  @user , you are correct that Reid certainly is...      0
1             Whoever just unfollowed me you a bitch      1
2  @user @user Those People Invaded Us!!! They DO...      1
3  stop JUDGING bitches by there cover, jus cuz s...      1
4  how about i knock heads off and send them gift...      1
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
len(train_labels) 9000
len(test_labels) 2970
len(val_labels) 1000

Unique values count in train_labels:
label
0    5217
1    3783
Name: count, dtype: int64

Unique values count in val_labels:
label
0    573
1    427
Name: count, dtype: int64

Unique values count in test_labels:
label
0    1718
1    1252
Name: count, dtype: int64





===================================================== 
flag 1.10  model:  started with ==>   bert
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8731

Evaluating...

Training Loss: 0.523
Validation Loss: 0.513
Validation Accuracy: 0.7480

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9463

Evaluating...

Training Loss: 0.342
Validation Loss: 0.503
Validation Accuracy: 0.7440

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9538

Evaluating...

Training Loss: 0.199
Validation Loss: 0.846
Validation Accuracy: 0.7430

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9829

Evaluating...

Training Loss: 0.144
Validation Loss: 0.870
Validation Accuracy: 0.7360

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9883

Evaluating...

Training Loss: 0.105
Validation Loss: 1.012
Validation Accuracy: 0.7460

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9888

Evaluating...

Training Loss: 0.093
Validation Loss: 1.010
Validation Accuracy: 0.7480

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9951

Evaluating...

Training Loss: 0.078
Validation Loss: 1.366
Validation Accuracy: 0.7400

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9910

Evaluating...

Training Loss: 0.061
Validation Loss: 1.193
Validation Accuracy: 0.7260

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9939

Evaluating...

Training Loss: 0.047
Validation Loss: 1.160
Validation Accuracy: 0.7480

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9918

Evaluating...

Training Loss: 0.069
Validation Loss: 1.493
Validation Accuracy: 0.7290

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9897

Evaluating...

Training Loss: 0.062
Validation Loss: 1.421
Validation Accuracy: 0.7570

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9933

Evaluating...

Training Loss: 0.061
Validation Loss: 1.427
Validation Accuracy: 0.7340

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9921

Evaluating...

Training Loss: 0.066
Validation Loss: 1.429
Validation Accuracy: 0.7390

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9950

Evaluating...

Training Loss: 0.048
Validation Loss: 1.517
Validation Accuracy: 0.7320

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9957

Evaluating...

Training Loss: 0.050
Validation Loss: 1.199
Validation Accuracy: 0.7320

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9951

Evaluating...

Training Loss: 0.046
Validation Loss: 1.200
Validation Accuracy: 0.7400

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9938

Evaluating...

Training Loss: 0.051
Validation Loss: 1.489
Validation Accuracy: 0.7120

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9959

Evaluating...

Training Loss: 0.050
Validation Loss: 1.718
Validation Accuracy: 0.7270

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9946

Evaluating...

Training Loss: 0.065
Validation Loss: 1.512
Validation Accuracy: 0.7310

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9904

Evaluating...

Training Loss: 0.086
Validation Loss: 1.667
Validation Accuracy: 0.7210

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9711

Evaluating...

Training Loss: 0.118
Validation Loss: 0.989
Validation Accuracy: 0.7030

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9722

Evaluating...

Training Loss: 0.138
Validation Loss: 0.924
Validation Accuracy: 0.7260

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9793

Evaluating...

Training Loss: 0.147
Validation Loss: 1.145
Validation Accuracy: 0.7050

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9728

Evaluating...

Training Loss: 0.175
Validation Loss: 1.041
Validation Accuracy: 0.7040

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9686

Evaluating...

Training Loss: 0.203
Validation Loss: 0.924
Validation Accuracy: 0.7020

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9471

Evaluating...

Training Loss: 0.239
Validation Loss: 0.730
Validation Accuracy: 0.7110

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9280

Evaluating...

Training Loss: 0.302
Validation Loss: 0.925
Validation Accuracy: 0.6520

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9369

Evaluating...

Training Loss: 0.337
Validation Loss: 0.848
Validation Accuracy: 0.7030

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8503

Evaluating...

Training Loss: 0.334
Validation Loss: 0.763
Validation Accuracy: 0.6790

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9237

Evaluating...

Training Loss: 0.354
Validation Loss: 0.872
Validation Accuracy: 0.6910


         == flag 1.601 bert result On test data ==
# called_model : bert
# Test Accuracy: 0.5138%
Precision: 0.6539
Recall: 0.5138
F1 Score: 0.4555
Classification Report:
              precision    recall  f1-score   support

           0       0.79      0.22      0.34      1718
           1       0.46      0.92      0.62      1252

    accuracy                           0.51      2970
   macro avg       0.63      0.57      0.48      2970
weighted avg       0.65      0.51      0.46      2970

Confusion Matrix:
[[ 370 1348]
 [  96 1156]]

flag 1.11  model:  finished  with:   bert





===================================================== 
flag 1.10  model:  started with ==>   roberta
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7911

Evaluating...

Training Loss: 0.570
Validation Loss: 0.524
Validation Accuracy: 0.7300

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7142

Evaluating...

Training Loss: 0.475
Validation Loss: 0.605
Validation Accuracy: 0.6030

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7950

Evaluating...

Training Loss: 0.502
Validation Loss: 0.626
Validation Accuracy: 0.6970

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7849

Evaluating...

Training Loss: 0.496
Validation Loss: 0.641
Validation Accuracy: 0.6710

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7470

Evaluating...

Training Loss: 0.505
Validation Loss: 0.667
Validation Accuracy: 0.6810

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7842

Evaluating...

Training Loss: 0.637
Validation Loss: 0.647
Validation Accuracy: 0.7030

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7836

Evaluating...

Training Loss: 0.561
Validation Loss: 0.663
Validation Accuracy: 0.6980

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6839

Evaluating...

Training Loss: 0.575
Validation Loss: 0.639
Validation Accuracy: 0.6460

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6563

Evaluating...

Training Loss: 0.603
Validation Loss: 0.644
Validation Accuracy: 0.6290

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6467

Evaluating...

Training Loss: 0.632
Validation Loss: 0.647
Validation Accuracy: 0.6260

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6509

Evaluating...

Training Loss: 0.632
Validation Loss: 0.649
Validation Accuracy: 0.6250

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6566

Evaluating...

Training Loss: 0.628
Validation Loss: 0.650
Validation Accuracy: 0.6280

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6608

Evaluating...

Training Loss: 0.620
Validation Loss: 0.653
Validation Accuracy: 0.6290

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7424

Evaluating...

Training Loss: 0.615
Validation Loss: 0.599
Validation Accuracy: 0.7030

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7106

Evaluating...

Training Loss: 0.534
Validation Loss: 0.623
Validation Accuracy: 0.6350

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.638
Validation Loss: 0.657
Validation Accuracy: 0.5730

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6743

Evaluating...

Training Loss: 0.649
Validation Loss: 0.640
Validation Accuracy: 0.6280

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6687

Evaluating...

Training Loss: 0.611
Validation Loss: 0.654
Validation Accuracy: 0.6100

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7150

Evaluating...

Training Loss: 0.609
Validation Loss: 0.667
Validation Accuracy: 0.6210

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.598
Validation Loss: 0.647
Validation Accuracy: 0.5730

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.639
Validation Loss: 0.648
Validation Accuracy: 0.5730

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.641
Validation Loss: 0.646
Validation Accuracy: 0.5730

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.637
Validation Loss: 0.648
Validation Accuracy: 0.5730

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.669
Validation Loss: 0.672
Validation Accuracy: 0.5730

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7249

Evaluating...

Training Loss: 0.653
Validation Loss: 0.645
Validation Accuracy: 0.6530

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.651
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 roberta result On test data ==
# called_model : roberta
# Test Accuracy: 0.4801%
Precision: 0.5900
Recall: 0.4801
F1 Score: 0.4140
Classification Report:
              precision    recall  f1-score   support

           0       0.70      0.18      0.28      1718
           1       0.44      0.89      0.59      1252

    accuracy                           0.48      2970
   macro avg       0.57      0.54      0.44      2970
weighted avg       0.59      0.48      0.41      2970

Confusion Matrix:
[[ 307 1411]
 [ 133 1119]]

flag 1.11  model:  finished  with:   roberta





===================================================== 
flag 1.10  model:  started with ==>   distilbert
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7971

Evaluating...

Training Loss: 0.490
Validation Loss: 0.546
Validation Accuracy: 0.7230

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9502

Evaluating...

Training Loss: 0.321
Validation Loss: 0.504
Validation Accuracy: 0.7610

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9778

Evaluating...

Training Loss: 0.179
Validation Loss: 0.840
Validation Accuracy: 0.7620

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9878

Evaluating...

Training Loss: 0.118
Validation Loss: 0.739
Validation Accuracy: 0.7440

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9932

Evaluating...

Training Loss: 0.077
Validation Loss: 1.047
Validation Accuracy: 0.7380

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9952

Evaluating...

Training Loss: 0.056
Validation Loss: 1.281
Validation Accuracy: 0.7460

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9959

Evaluating...

Training Loss: 0.046
Validation Loss: 1.359
Validation Accuracy: 0.7440

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9962

Evaluating...

Training Loss: 0.039
Validation Loss: 1.475
Validation Accuracy: 0.7620

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9974

Evaluating...

Training Loss: 0.034
Validation Loss: 1.722
Validation Accuracy: 0.7340

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9974

Evaluating...

Training Loss: 0.026
Validation Loss: 1.531
Validation Accuracy: 0.7450

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9979

Evaluating...

Training Loss: 0.029
Validation Loss: 1.711
Validation Accuracy: 0.7520

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9969

Evaluating...

Training Loss: 0.022
Validation Loss: 1.369
Validation Accuracy: 0.7500

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.023
Validation Loss: 1.741
Validation Accuracy: 0.7540

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9976

Evaluating...

Training Loss: 0.025
Validation Loss: 1.463
Validation Accuracy: 0.7290

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9906

Evaluating...

Training Loss: 0.028
Validation Loss: 1.719
Validation Accuracy: 0.7340

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9966

Evaluating...

Training Loss: 0.030
Validation Loss: 1.460
Validation Accuracy: 0.7460

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9981

Evaluating...

Training Loss: 0.030
Validation Loss: 1.642
Validation Accuracy: 0.7270

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.027
Validation Loss: 1.971
Validation Accuracy: 0.7240

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.016
Validation Loss: 2.094
Validation Accuracy: 0.7270

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9984

Evaluating...

Training Loss: 0.023
Validation Loss: 1.710
Validation Accuracy: 0.7350

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9972

Evaluating...

Training Loss: 0.022
Validation Loss: 1.516
Validation Accuracy: 0.7380

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9982

Evaluating...

Training Loss: 0.017
Validation Loss: 1.916
Validation Accuracy: 0.7390

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9981

Evaluating...

Training Loss: 0.018
Validation Loss: 1.768
Validation Accuracy: 0.7290

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.022
Validation Loss: 1.636
Validation Accuracy: 0.7350

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9969

Evaluating...

Training Loss: 0.022
Validation Loss: 1.757
Validation Accuracy: 0.7350

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9946

Evaluating...

Training Loss: 0.022
Validation Loss: 1.553
Validation Accuracy: 0.7080

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.027
Validation Loss: 1.778
Validation Accuracy: 0.7350

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.023
Validation Loss: 1.924
Validation Accuracy: 0.7280

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9951

Evaluating...

Training Loss: 0.044
Validation Loss: 1.146
Validation Accuracy: 0.7390

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9972

Evaluating...

Training Loss: 0.036
Validation Loss: 1.518
Validation Accuracy: 0.7130


         == flag 1.601 distilbert result On test data ==
# called_model : distilbert
# Test Accuracy: 0.4771%
Precision: 0.6566
Recall: 0.4771
F1 Score: 0.3815
Classification Report:
              precision    recall  f1-score   support

           0       0.81      0.13      0.22      1718
           1       0.44      0.96      0.61      1252

    accuracy                           0.48      2970
   macro avg       0.63      0.54      0.41      2970
weighted avg       0.66      0.48      0.38      2970

Confusion Matrix:
[[ 215 1503]
 [  50 1202]]

flag 1.11  model:  finished  with:   distilbert





===================================================== 
flag 1.10  model:  started with ==>   electra
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7752

Evaluating...

Training Loss: 0.529
Validation Loss: 0.657
Validation Accuracy: 0.6770

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8559

Evaluating...

Training Loss: 0.407
Validation Loss: 0.577
Validation Accuracy: 0.7090

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9066

Evaluating...

Training Loss: 0.387
Validation Loss: 0.619
Validation Accuracy: 0.7610

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8361

Evaluating...

Training Loss: 0.365
Validation Loss: 0.648
Validation Accuracy: 0.6450

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9391

Evaluating...

Training Loss: 0.280
Validation Loss: 0.718
Validation Accuracy: 0.7560

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9358

Evaluating...

Training Loss: 0.233
Validation Loss: 0.742
Validation Accuracy: 0.7270

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9564

Evaluating...

Training Loss: 0.212
Validation Loss: 0.746
Validation Accuracy: 0.7570

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9527

Evaluating...

Training Loss: 0.210
Validation Loss: 0.939
Validation Accuracy: 0.7280

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9682

Evaluating...

Training Loss: 0.161
Validation Loss: 0.799
Validation Accuracy: 0.7320

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9694

Evaluating...

Training Loss: 0.162
Validation Loss: 1.091
Validation Accuracy: 0.7200

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9679

Evaluating...

Training Loss: 0.144
Validation Loss: 0.943
Validation Accuracy: 0.7390

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9686

Evaluating...

Training Loss: 0.167
Validation Loss: 0.963
Validation Accuracy: 0.7450

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9621

Evaluating...

Training Loss: 0.162
Validation Loss: 0.980
Validation Accuracy: 0.7230

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9624

Evaluating...

Training Loss: 0.189
Validation Loss: 0.882
Validation Accuracy: 0.7300

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9637

Evaluating...

Training Loss: 0.188
Validation Loss: 0.902
Validation Accuracy: 0.7420

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9514

Evaluating...

Training Loss: 0.246
Validation Loss: 0.867
Validation Accuracy: 0.7310

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9214

Evaluating...

Training Loss: 0.262
Validation Loss: 0.992
Validation Accuracy: 0.7140

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9499

Evaluating...

Training Loss: 0.279
Validation Loss: 0.936
Validation Accuracy: 0.7160

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9471

Evaluating...

Training Loss: 0.295
Validation Loss: 0.877
Validation Accuracy: 0.7070

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9072

Evaluating...

Training Loss: 0.288
Validation Loss: 0.815
Validation Accuracy: 0.6710

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9321

Evaluating...

Training Loss: 0.271
Validation Loss: 0.926
Validation Accuracy: 0.7010

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9389

Evaluating...

Training Loss: 0.278
Validation Loss: 0.889
Validation Accuracy: 0.7040

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9028

Evaluating...

Training Loss: 0.321
Validation Loss: 0.772
Validation Accuracy: 0.7060

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8456

Evaluating...

Training Loss: 0.345
Validation Loss: 0.966
Validation Accuracy: 0.6470

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.473
Validation Loss: 0.689
Validation Accuracy: 0.5730

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.689
Validation Accuracy: 0.5730

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 electra result On test data ==
# called_model : electra
# Test Accuracy: 0.4475%
Precision: 0.6919
Recall: 0.4475
F1 Score: 0.3103
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.05      0.10      1718
           1       0.43      0.99      0.60      1252

    accuracy                           0.45      2970
   macro avg       0.66      0.52      0.35      2970
weighted avg       0.69      0.45      0.31      2970

Confusion Matrix:
[[  89 1629]
 [  12 1240]]

flag 1.11  model:  finished  with:   electra





===================================================== 
flag 1.10  model:  started with ==>   gpt2
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8157

Evaluating...

Training Loss: 0.533
Validation Loss: 0.590
Validation Accuracy: 0.7390

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8832

Evaluating...

Training Loss: 0.400
Validation Loss: 0.510
Validation Accuracy: 0.7400

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9552

Evaluating...

Training Loss: 0.288
Validation Loss: 0.597
Validation Accuracy: 0.7420

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9722

Evaluating...

Training Loss: 0.183
Validation Loss: 0.737
Validation Accuracy: 0.7470

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9922

Evaluating...

Training Loss: 0.102
Validation Loss: 0.890
Validation Accuracy: 0.7560

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9924

Evaluating...

Training Loss: 0.072
Validation Loss: 1.217
Validation Accuracy: 0.7310

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9897

Evaluating...

Training Loss: 0.050
Validation Loss: 1.436
Validation Accuracy: 0.7490

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9918

Evaluating...

Training Loss: 0.044
Validation Loss: 1.609
Validation Accuracy: 0.7330

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9919

Evaluating...

Training Loss: 0.035
Validation Loss: 1.680
Validation Accuracy: 0.7400

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9981

Evaluating...

Training Loss: 0.038
Validation Loss: 1.340
Validation Accuracy: 0.7540

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.029
Validation Loss: 1.517
Validation Accuracy: 0.7660

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.033
Validation Loss: 1.762
Validation Accuracy: 0.7630

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9984

Evaluating...

Training Loss: 0.026
Validation Loss: 1.748
Validation Accuracy: 0.7640

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.023
Validation Loss: 1.588
Validation Accuracy: 0.7650

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.022
Validation Loss: 1.595
Validation Accuracy: 0.7630

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9979

Evaluating...

Training Loss: 0.023
Validation Loss: 1.698
Validation Accuracy: 0.7650

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.024
Validation Loss: 1.823
Validation Accuracy: 0.7500

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.020
Validation Loss: 1.697
Validation Accuracy: 0.7670

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.018
Validation Loss: 1.982
Validation Accuracy: 0.7490

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.024
Validation Loss: 1.866
Validation Accuracy: 0.7580

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.023
Validation Loss: 1.458
Validation Accuracy: 0.7630

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9924

Evaluating...

Training Loss: 0.020
Validation Loss: 2.072
Validation Accuracy: 0.7390

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.019
Validation Loss: 2.308
Validation Accuracy: 0.7480

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9968

Evaluating...

Training Loss: 0.017
Validation Loss: 1.864
Validation Accuracy: 0.7760

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.020
Validation Loss: 1.892
Validation Accuracy: 0.7540

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.015
Validation Loss: 1.725
Validation Accuracy: 0.7660

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.018
Validation Loss: 1.456
Validation Accuracy: 0.7730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.017
Validation Loss: 1.789
Validation Accuracy: 0.7540

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.014
Validation Loss: 1.812
Validation Accuracy: 0.7590

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.014
Validation Loss: 1.687
Validation Accuracy: 0.7590


         == flag 1.601 gpt2 result On test data ==
# called_model : gpt2
# Test Accuracy: 0.5104%
Precision: 0.6324
Recall: 0.5104
F1 Score: 0.4574
Classification Report:
              precision    recall  f1-score   support

           0       0.76      0.23      0.35      1718
           1       0.46      0.90      0.61      1252

    accuracy                           0.51      2970
   macro avg       0.61      0.56      0.48      2970
weighted avg       0.63      0.51      0.46      2970

Confusion Matrix:
[[ 387 1331]
 [ 123 1129]]

flag 1.11  model:  finished  with:   gpt2





===================================================== 
flag 1.10  model:  started with ==>   longformer
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7332

Evaluating...

Training Loss: 0.591
Validation Loss: 0.573
Validation Accuracy: 0.6820

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5807

Evaluating...

Training Loss: 0.523
Validation Loss: 0.646
Validation Accuracy: 0.5380

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7863

Evaluating...

Training Loss: 0.525
Validation Loss: 0.784
Validation Accuracy: 0.6980

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7169

Evaluating...

Training Loss: 0.629
Validation Loss: 0.638
Validation Accuracy: 0.6040

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6864

Evaluating...

Training Loss: 0.552
Validation Loss: 0.631
Validation Accuracy: 0.6010

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 longformer result On test data ==
# called_model : longformer
# Test Accuracy: 0.4912%
Precision: 0.5438
Recall: 0.4912
F1 Score: 0.4714
Classification Report:
              precision    recall  f1-score   support

           0       0.62      0.31      0.41      1718
           1       0.44      0.74      0.55      1252

    accuracy                           0.49      2970
   macro avg       0.53      0.52      0.48      2970
weighted avg       0.54      0.49      0.47      2970

Confusion Matrix:
[[ 533 1185]
 [ 326  926]]

flag 1.11  model:  finished  with:   longformer





===================================================== 
flag 1.10  model:  started with ==>   luke
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7747

Evaluating...

Training Loss: 0.583
Validation Loss: 0.619
Validation Accuracy: 0.6910

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5794

Evaluating...

Training Loss: 0.630
Validation Loss: 0.675
Validation Accuracy: 0.5730

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.4203

Evaluating...

Training Loss: 0.658
Validation Loss: 0.937
Validation Accuracy: 0.4270

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.687
Validation Loss: 0.676
Validation Accuracy: 0.5730

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.676
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730


         == flag 1.601 luke result On test data ==
# called_model : luke
# Test Accuracy: 0.4397%
Precision: 0.5777
Recall: 0.4397
F1 Score: 0.3110
Classification Report:
              precision    recall  f1-score   support

           0       0.69      0.06      0.11      1718
           1       0.43      0.96      0.59      1252

    accuracy                           0.44      2970
   macro avg       0.56      0.51      0.35      2970
weighted avg       0.58      0.44      0.31      2970

Confusion Matrix:
[[  99 1619]
 [  45 1207]]

flag 1.11  model:  finished  with:   luke





===================================================== 
flag 1.10  model:  started with ==>   t5
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8509

Evaluating...

Training Loss: 0.586
Validation Loss: 0.503
Validation Accuracy: 0.7340

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9162

Evaluating...

Training Loss: 0.372
Validation Loss: 0.515
Validation Accuracy: 0.7700

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9477

Evaluating...

Training Loss: 0.274
Validation Loss: 0.639
Validation Accuracy: 0.7510

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9819

Evaluating...

Training Loss: 0.185
Validation Loss: 0.727
Validation Accuracy: 0.7710

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9873

Evaluating...

Training Loss: 0.124
Validation Loss: 1.018
Validation Accuracy: 0.7490

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9911

Evaluating...

Training Loss: 0.099
Validation Loss: 1.134
Validation Accuracy: 0.7430

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9959

Evaluating...

Training Loss: 0.068
Validation Loss: 1.215
Validation Accuracy: 0.7560

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9958

Evaluating...

Training Loss: 0.050
Validation Loss: 1.446
Validation Accuracy: 0.7550

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9944

Evaluating...

Training Loss: 0.042
Validation Loss: 1.659
Validation Accuracy: 0.7500

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.041
Validation Loss: 1.506
Validation Accuracy: 0.7550

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9971

Evaluating...

Training Loss: 0.030
Validation Loss: 1.737
Validation Accuracy: 0.7600

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.029
Validation Loss: 1.729
Validation Accuracy: 0.7450

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9984

Evaluating...

Training Loss: 0.022
Validation Loss: 1.762
Validation Accuracy: 0.7620

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.025
Validation Loss: 1.814
Validation Accuracy: 0.7600

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.021
Validation Loss: 1.679
Validation Accuracy: 0.7620

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.019
Validation Loss: 1.642
Validation Accuracy: 0.7750

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.022
Validation Loss: 1.916
Validation Accuracy: 0.7500

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.024
Validation Loss: 1.765
Validation Accuracy: 0.7600

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.025
Validation Loss: 1.661
Validation Accuracy: 0.7640

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.016
Validation Loss: 1.750
Validation Accuracy: 0.7720

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.023
Validation Loss: 1.651
Validation Accuracy: 0.7700

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.019
Validation Loss: 1.579
Validation Accuracy: 0.7670

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.015
Validation Loss: 2.007
Validation Accuracy: 0.7570

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.013
Validation Loss: 1.742
Validation Accuracy: 0.7850

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.020
Validation Loss: 1.898
Validation Accuracy: 0.7640

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.018
Validation Loss: 1.828
Validation Accuracy: 0.7660

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9982

Evaluating...

Training Loss: 0.020
Validation Loss: 1.857
Validation Accuracy: 0.7500

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.018
Validation Loss: 1.708
Validation Accuracy: 0.7740

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.014
Validation Loss: 1.769
Validation Accuracy: 0.7740

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.017
Validation Loss: 1.769
Validation Accuracy: 0.7650


         == flag 1.601 t5 result On test data ==
# called_model : t5
# Test Accuracy: 0.4764%
Precision: 0.6863
Recall: 0.4764
F1 Score: 0.3731
Classification Report:
              precision    recall  f1-score   support

           0       0.86      0.11      0.20      1718
           1       0.44      0.98      0.61      1252

    accuracy                           0.48      2970
   macro avg       0.65      0.54      0.41      2970
weighted avg       0.69      0.48      0.37      2970

Confusion Matrix:
[[ 194 1524]
 [  31 1221]]

flag 1.11  model:  finished  with:   t5





===================================================== 
flag 1.10  model:  started with ==>   xlnet
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8331

Evaluating...

Training Loss: 0.560
Validation Loss: 0.522
Validation Accuracy: 0.7440

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8768

Evaluating...

Training Loss: 0.415
Validation Loss: 0.681
Validation Accuracy: 0.6940

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8896

Evaluating...

Training Loss: 0.353
Validation Loss: 0.590
Validation Accuracy: 0.7400

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9072

Evaluating...

Training Loss: 0.349
Validation Loss: 0.769
Validation Accuracy: 0.7000

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8703

Evaluating...

Training Loss: 0.382
Validation Loss: 0.736
Validation Accuracy: 0.7120

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8752

Evaluating...

Training Loss: 0.411
Validation Loss: 0.901
Validation Accuracy: 0.6980

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7599

Evaluating...

Training Loss: 0.399
Validation Loss: 0.705
Validation Accuracy: 0.6480

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7174

Evaluating...

Training Loss: 0.604
Validation Loss: 0.676
Validation Accuracy: 0.6520

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7761

Evaluating...

Training Loss: 0.537
Validation Loss: 0.689
Validation Accuracy: 0.6700

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6510

Evaluating...

Training Loss: 0.534
Validation Loss: 0.668
Validation Accuracy: 0.5550

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.657
Validation Loss: 0.681
Validation Accuracy: 0.5730

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.688
Validation Accuracy: 0.5730

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.672
Validation Loss: 0.681
Validation Accuracy: 0.5730

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.6077

Evaluating...

Training Loss: 0.670
Validation Loss: 0.689
Validation Accuracy: 0.5910

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.710
Validation Accuracy: 0.5730

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.718
Validation Accuracy: 0.5730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.738
Validation Accuracy: 0.5730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.679
Validation Loss: 0.722
Validation Accuracy: 0.5730

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.678
Validation Loss: 0.683
Validation Accuracy: 0.5730


         == flag 1.601 xlnet result On test data ==
# called_model : xlnet
# Test Accuracy: 0.5071%
Precision: 0.6098
Recall: 0.5071
F1 Score: 0.4611
Classification Report:
              precision    recall  f1-score   support

           0       0.72      0.24      0.36      1718
           1       0.46      0.87      0.60      1252

    accuracy                           0.51      2970
   macro avg       0.59      0.56      0.48      2970
weighted avg       0.61      0.51      0.46      2970

Confusion Matrix:
[[ 413 1305]
 [ 159 1093]]

flag 1.11  model:  finished  with:   xlnet
