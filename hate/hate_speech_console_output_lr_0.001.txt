learning rate  : 0.001
epochs : 20
                                                text  label
0  @user nice new signage. Are you not concerned ...      0
1  A woman who you fucked multiple times saying y...      1
2  @user @user real talk do you have eyes or were...      1
3  your girlfriend lookin at me like a groupie in...      1
4                        Hysterical woman like @user      0
                                                text
0  @user nice new signage. Are you not concerned ...
1  A woman who you fucked multiple times saying y...
2  @user @user real talk do you have eyes or were...
3  your girlfriend lookin at me like a groupie in...
4                        Hysterical woman like @user
   label
0      0
1      1
2      1
3      1
4      0
                                                text  label
0  @user @user If book Claire wanted to "stay in ...      0
1  After arriving in the EU refugees make protest...      0
2                                                 ðŸ˜³ðŸ‘‡      0
3  @user Worst thing is if they are that stupid t...      1
4  @user Say's the HYSTERICAL woman. It is woman ...      0
                                                text
0  @user @user If book Claire wanted to "stay in ...
1  After arriving in the EU refugees make protest...
2                                                 ðŸ˜³ðŸ‘‡
3  @user Worst thing is if they are that stupid t...
4  @user Say's the HYSTERICAL woman. It is woman ...
   label
0      0
1      0
2      0
3      1
4      0
                                                text  label
0  @user , you are correct that Reid certainly is...      0
1             Whoever just unfollowed me you a bitch      1
2  @user @user Those People Invaded Us!!! They DO...      1
3  stop JUDGING bitches by there cover, jus cuz s...      1
4  how about i knock heads off and send them gift...      1
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
len(train_labels) 9000
len(test_labels) 2970
len(val_labels) 1000

Unique values count in train_labels:
label
0    5217
1    3783
Name: count, dtype: int64

Unique values count in val_labels:
label
0    573
1    427
Name: count, dtype: int64

Unique values count in test_labels:
label
0    1718
1    1252
Name: count, dtype: int64





===================================================== 
flag 1.10  model:  started with ==>   bert
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.732
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.697
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.690
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.687
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.685
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 bert result On test data ==
# called_model : bert
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   bert





===================================================== 
flag 1.10  model:  started with ==>   roberta
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.4203

Evaluating...

Training Loss: 0.709
Validation Loss: 0.716
Validation Accuracy: 0.4270

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.690
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.685
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.691
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.4203

Evaluating...

Training Loss: 0.682
Validation Loss: 0.693
Validation Accuracy: 0.4270

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.690
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.686
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.688
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730


         == flag 1.601 roberta result On test data ==
# called_model : roberta
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   roberta





===================================================== 
flag 1.10  model:  started with ==>   distilbert
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.701
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.685
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730


         == flag 1.601 distilbert result On test data ==
# called_model : distilbert
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   distilbert





===================================================== 
flag 1.10  model:  started with ==>   electra
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.700
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.687
Validation Loss: 0.688
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.732
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.691
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 electra result On test data ==
# called_model : electra
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   electra





===================================================== 
flag 1.10  model:  started with ==>   gpt2
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8277

Evaluating...

Training Loss: 0.565
Validation Loss: 0.561
Validation Accuracy: 0.7200

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9369

Evaluating...

Training Loss: 0.368
Validation Loss: 0.686
Validation Accuracy: 0.7240

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9710

Evaluating...

Training Loss: 0.215
Validation Loss: 0.782
Validation Accuracy: 0.7010

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9780

Evaluating...

Training Loss: 0.112
Validation Loss: 1.162
Validation Accuracy: 0.6730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9907

Evaluating...

Training Loss: 0.064
Validation Loss: 1.108
Validation Accuracy: 0.6770

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9957

Evaluating...

Training Loss: 0.041
Validation Loss: 1.787
Validation Accuracy: 0.6750

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9957

Evaluating...

Training Loss: 0.029
Validation Loss: 1.877
Validation Accuracy: 0.6590

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.019
Validation Loss: 2.274
Validation Accuracy: 0.6590

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9927

Evaluating...

Training Loss: 0.017
Validation Loss: 2.094
Validation Accuracy: 0.6920

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9984

Evaluating...

Training Loss: 0.021
Validation Loss: 2.178
Validation Accuracy: 0.6840

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9983

Evaluating...

Training Loss: 0.022
Validation Loss: 2.358
Validation Accuracy: 0.6730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.016
Validation Loss: 2.428
Validation Accuracy: 0.6810

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.019
Validation Loss: 2.112
Validation Accuracy: 0.6680

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9973

Evaluating...

Training Loss: 0.018
Validation Loss: 1.718
Validation Accuracy: 0.6730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9959

Evaluating...

Training Loss: 0.022
Validation Loss: 2.835
Validation Accuracy: 0.6680

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.013
Validation Loss: 4.748
Validation Accuracy: 0.6950

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.012
Validation Loss: 3.368
Validation Accuracy: 0.6770

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9950

Evaluating...

Training Loss: 0.014
Validation Loss: 4.392
Validation Accuracy: 0.6460

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.015
Validation Loss: 2.098
Validation Accuracy: 0.6730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.010
Validation Loss: 4.096
Validation Accuracy: 0.6720


         == flag 1.601 gpt2 result On test data ==
# called_model : gpt2
# Test Accuracy: 0.4579%
Precision: 0.5463
Recall: 0.4579
F1 Score: 0.3846
Classification Report:
              precision    recall  f1-score   support

           0       0.63      0.15      0.24      1718
           1       0.43      0.88      0.58      1252

    accuracy                           0.46      2970
   macro avg       0.53      0.51      0.41      2970
weighted avg       0.55      0.46      0.38      2970

Confusion Matrix:
[[ 260 1458]
 [ 152 1100]]

flag 1.11  model:  finished  with:   gpt2





===================================================== 
flag 1.10  model:  started with ==>   longformer
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.701
Validation Loss: 0.696
Validation Accuracy: 0.5730

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.687
Validation Loss: 0.688
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.687
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.718
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.694
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730


         == flag 1.601 longformer result On test data ==
# called_model : longformer
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   longformer





===================================================== 
flag 1.10  model:  started with ==>   luke
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.4203

Evaluating...

Training Loss: 0.705
Validation Loss: 0.693
Validation Accuracy: 0.4270

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.687
Validation Loss: 0.688
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.693
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.698
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.684
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.680
Validation Loss: 0.686
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730


         == flag 1.601 luke result On test data ==
# called_model : luke
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   luke





===================================================== 
flag 1.10  model:  started with ==>   t5
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8431

Evaluating...

Training Loss: 0.521
Validation Loss: 0.509
Validation Accuracy: 0.7510

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8896

Evaluating...

Training Loss: 0.390
Validation Loss: 0.552
Validation Accuracy: 0.7550

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9414

Evaluating...

Training Loss: 0.321
Validation Loss: 0.525
Validation Accuracy: 0.7320

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9592

Evaluating...

Training Loss: 0.212
Validation Loss: 0.664
Validation Accuracy: 0.7230

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9746

Evaluating...

Training Loss: 0.157
Validation Loss: 1.035
Validation Accuracy: 0.7020

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9891

Evaluating...

Training Loss: 0.131
Validation Loss: 0.927
Validation Accuracy: 0.7180

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9913

Evaluating...

Training Loss: 0.106
Validation Loss: 1.028
Validation Accuracy: 0.7330

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9910

Evaluating...

Training Loss: 0.079
Validation Loss: 1.266
Validation Accuracy: 0.7040

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9939

Evaluating...

Training Loss: 0.074
Validation Loss: 1.218
Validation Accuracy: 0.7450

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9908

Evaluating...

Training Loss: 0.056
Validation Loss: 1.331
Validation Accuracy: 0.7070

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9911

Evaluating...

Training Loss: 0.054
Validation Loss: 1.524
Validation Accuracy: 0.7130

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9821

Evaluating...

Training Loss: 0.055
Validation Loss: 1.228
Validation Accuracy: 0.7220

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9950

Evaluating...

Training Loss: 0.055
Validation Loss: 1.451
Validation Accuracy: 0.7390

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9966

Evaluating...

Training Loss: 0.044
Validation Loss: 1.502
Validation Accuracy: 0.7410

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9961

Evaluating...

Training Loss: 0.035
Validation Loss: 1.423
Validation Accuracy: 0.7360

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9961

Evaluating...

Training Loss: 0.051
Validation Loss: 1.383
Validation Accuracy: 0.7460

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9960

Evaluating...

Training Loss: 0.048
Validation Loss: 1.683
Validation Accuracy: 0.7300

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9954

Evaluating...

Training Loss: 0.041
Validation Loss: 1.262
Validation Accuracy: 0.7410

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9918

Evaluating...

Training Loss: 0.040
Validation Loss: 1.944
Validation Accuracy: 0.7180

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9982

Evaluating...

Training Loss: 0.042
Validation Loss: 1.576
Validation Accuracy: 0.7350


         == flag 1.601 t5 result On test data ==
# called_model : t5
# Test Accuracy: 0.4569%
Precision: 0.5745
Recall: 0.4569
F1 Score: 0.3633
Classification Report:
              precision    recall  f1-score   support

           0       0.68      0.12      0.20      1718
           1       0.43      0.92      0.59      1252

    accuracy                           0.46      2970
   macro avg       0.56      0.52      0.39      2970
weighted avg       0.57      0.46      0.36      2970

Confusion Matrix:
[[ 200 1518]
 [  95 1157]]

flag 1.11  model:  finished  with:   t5





===================================================== 
flag 1.10  model:  started with ==>   xlnet
===================================================== 

 Epoch 1 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.740
Validation Loss: 0.688
Validation Accuracy: 0.5730

 Epoch 2 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.689
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 3 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 4 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.696
Validation Loss: 0.686
Validation Accuracy: 0.5730

 Epoch 5 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.684
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 6 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 7 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 8 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 9 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 10 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 11 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.685
Validation Accuracy: 0.5730

 Epoch 12 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 13 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 14 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 15 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 16 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.683
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 17 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 18 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.682
Validation Accuracy: 0.5730

 Epoch 19 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.681
Validation Loss: 0.683
Validation Accuracy: 0.5730

 Epoch 20 / 20
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.682
Validation Loss: 0.684
Validation Accuracy: 0.5730


         == flag 1.601 xlnet result On test data ==
# called_model : xlnet
# Test Accuracy: 0.5785%
Precision: 0.3346
Recall: 0.5785
F1 Score: 0.4240
Classification Report:
              precision    recall  f1-score   support

           0       0.58      1.00      0.73      1718
           1       0.00      0.00      0.00      1252

    accuracy                           0.58      2970
   macro avg       0.29      0.50      0.37      2970
weighted avg       0.33      0.58      0.42      2970

Confusion Matrix:
[[1718    0]
 [1252    0]]

flag 1.11  model:  finished  with:   xlnet
