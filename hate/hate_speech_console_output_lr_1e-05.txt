learning rate  : 1e-05
epochs : 30
                                                text  label
0  @user nice new signage. Are you not concerned ...      0
1  A woman who you fucked multiple times saying y...      1
2  @user @user real talk do you have eyes or were...      1
3  your girlfriend lookin at me like a groupie in...      1
4                        Hysterical woman like @user      0
                                                text
0  @user nice new signage. Are you not concerned ...
1  A woman who you fucked multiple times saying y...
2  @user @user real talk do you have eyes or were...
3  your girlfriend lookin at me like a groupie in...
4                        Hysterical woman like @user
   label
0      0
1      1
2      1
3      1
4      0
                                                text  label
0  @user @user If book Claire wanted to "stay in ...      0
1  After arriving in the EU refugees make protest...      0
2                                                 ðŸ˜³ðŸ‘‡      0
3  @user Worst thing is if they are that stupid t...      1
4  @user Say's the HYSTERICAL woman. It is woman ...      0
                                                text
0  @user @user If book Claire wanted to "stay in ...
1  After arriving in the EU refugees make protest...
2                                                 ðŸ˜³ðŸ‘‡
3  @user Worst thing is if they are that stupid t...
4  @user Say's the HYSTERICAL woman. It is woman ...
   label
0      0
1      0
2      0
3      1
4      0
                                                text  label
0  @user , you are correct that Reid certainly is...      0
1             Whoever just unfollowed me you a bitch      1
2  @user @user Those People Invaded Us!!! They DO...      1
3  stop JUDGING bitches by there cover, jus cuz s...      1
4  how about i knock heads off and send them gift...      1
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
                                                text
0  @user , you are correct that Reid certainly is...
1             Whoever just unfollowed me you a bitch
2  @user @user Those People Invaded Us!!! They DO...
3  stop JUDGING bitches by there cover, jus cuz s...
4  how about i knock heads off and send them gift...
len(train_labels) 9000
len(test_labels) 2970
len(val_labels) 1000

Unique values count in train_labels:
label
0    5217
1    3783
Name: count, dtype: int64

Unique values count in val_labels:
label
0    573
1    427
Name: count, dtype: int64

Unique values count in test_labels:
label
0    1718
1    1252
Name: count, dtype: int64





===================================================== 
flag 1.10  model:  started with ==>   bert
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8358

Evaluating...

Training Loss: 0.662
Validation Loss: 0.537
Validation Accuracy: 0.7610

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9013

Evaluating...

Training Loss: 0.392
Validation Loss: 0.500
Validation Accuracy: 0.7750

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9577

Evaluating...

Training Loss: 0.288
Validation Loss: 0.524
Validation Accuracy: 0.7740

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9777

Evaluating...

Training Loss: 0.200
Validation Loss: 0.587
Validation Accuracy: 0.7930

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9910

Evaluating...

Training Loss: 0.133
Validation Loss: 0.814
Validation Accuracy: 0.7760

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9930

Evaluating...

Training Loss: 0.099
Validation Loss: 0.894
Validation Accuracy: 0.7860

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9942

Evaluating...

Training Loss: 0.079
Validation Loss: 1.024
Validation Accuracy: 0.7690

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9961

Evaluating...

Training Loss: 0.059
Validation Loss: 1.080
Validation Accuracy: 0.7740

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9972

Evaluating...

Training Loss: 0.050
Validation Loss: 1.131
Validation Accuracy: 0.7830

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.043
Validation Loss: 1.268
Validation Accuracy: 0.7750

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9978

Evaluating...

Training Loss: 0.037
Validation Loss: 1.267
Validation Accuracy: 0.7860

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.035
Validation Loss: 1.213
Validation Accuracy: 0.7960

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.027
Validation Loss: 1.331
Validation Accuracy: 0.7940

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.024
Validation Loss: 1.357
Validation Accuracy: 0.7850

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.023
Validation Loss: 1.546
Validation Accuracy: 0.7710

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9982

Evaluating...

Training Loss: 0.027
Validation Loss: 1.645
Validation Accuracy: 0.7640

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.026
Validation Loss: 1.461
Validation Accuracy: 0.7840

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.014
Validation Loss: 1.586
Validation Accuracy: 0.7700

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.017
Validation Loss: 1.446
Validation Accuracy: 0.7880

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.021
Validation Loss: 1.457
Validation Accuracy: 0.7830

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.022
Validation Loss: 1.604
Validation Accuracy: 0.7820

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.017
Validation Loss: 1.575
Validation Accuracy: 0.7910

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.015
Validation Loss: 1.605
Validation Accuracy: 0.7830

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.013
Validation Loss: 1.950
Validation Accuracy: 0.7520

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.013
Validation Loss: 1.699
Validation Accuracy: 0.7840

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.017
Validation Loss: 1.607
Validation Accuracy: 0.7800

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.020
Validation Loss: 1.517
Validation Accuracy: 0.7860

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.011
Validation Loss: 1.710
Validation Accuracy: 0.7770

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.009
Validation Loss: 1.775
Validation Accuracy: 0.7880

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.011
Validation Loss: 1.697
Validation Accuracy: 0.7950


         == flag 1.601 bert result On test data ==
# called_model : bert
# Test Accuracy: 0.4993%
Precision: 0.7160
Recall: 0.4993
F1 Score: 0.4113
Classification Report:
              precision    recall  f1-score   support

           0       0.91      0.15      0.26      1718
           1       0.46      0.98      0.62      1252

    accuracy                           0.50      2970
   macro avg       0.68      0.56      0.44      2970
weighted avg       0.72      0.50      0.41      2970

Confusion Matrix:
[[ 258 1460]
 [  27 1225]]

flag 1.11  model:  finished  with:   bert





===================================================== 
flag 1.10  model:  started with ==>   roberta
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8226

Evaluating...

Training Loss: 0.632
Validation Loss: 0.540
Validation Accuracy: 0.7260

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8522

Evaluating...

Training Loss: 0.402
Validation Loss: 0.531
Validation Accuracy: 0.7530

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9190

Evaluating...

Training Loss: 0.327
Validation Loss: 0.497
Validation Accuracy: 0.7820

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9493

Evaluating...

Training Loss: 0.267
Validation Loss: 0.542
Validation Accuracy: 0.7860

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9683

Evaluating...

Training Loss: 0.220
Validation Loss: 0.578
Validation Accuracy: 0.7820

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9768

Evaluating...

Training Loss: 0.185
Validation Loss: 0.599
Validation Accuracy: 0.7790

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9817

Evaluating...

Training Loss: 0.149
Validation Loss: 0.675
Validation Accuracy: 0.7940

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9892

Evaluating...

Training Loss: 0.127
Validation Loss: 0.774
Validation Accuracy: 0.7960

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9820

Evaluating...

Training Loss: 0.119
Validation Loss: 1.098
Validation Accuracy: 0.7560

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9814

Evaluating...

Training Loss: 0.096
Validation Loss: 1.043
Validation Accuracy: 0.7810

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9941

Evaluating...

Training Loss: 0.085
Validation Loss: 1.203
Validation Accuracy: 0.7830

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9940

Evaluating...

Training Loss: 0.079
Validation Loss: 1.319
Validation Accuracy: 0.7690

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9964

Evaluating...

Training Loss: 0.059
Validation Loss: 1.146
Validation Accuracy: 0.7880

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9973

Evaluating...

Training Loss: 0.055
Validation Loss: 1.268
Validation Accuracy: 0.7840

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9967

Evaluating...

Training Loss: 0.051
Validation Loss: 1.507
Validation Accuracy: 0.7780

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.063
Validation Loss: 1.276
Validation Accuracy: 0.7810

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9963

Evaluating...

Training Loss: 0.038
Validation Loss: 1.635
Validation Accuracy: 0.7710

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.046
Validation Loss: 1.533
Validation Accuracy: 0.7690

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.049
Validation Loss: 1.509
Validation Accuracy: 0.7660

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.038
Validation Loss: 1.572
Validation Accuracy: 0.7780

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9983

Evaluating...

Training Loss: 0.034
Validation Loss: 1.823
Validation Accuracy: 0.7760

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.030
Validation Loss: 1.532
Validation Accuracy: 0.7880

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.041
Validation Loss: 1.762
Validation Accuracy: 0.7710

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9979

Evaluating...

Training Loss: 0.024
Validation Loss: 1.880
Validation Accuracy: 0.7590

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.033
Validation Loss: 1.611
Validation Accuracy: 0.7960

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.026
Validation Loss: 1.618
Validation Accuracy: 0.7810

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9947

Evaluating...

Training Loss: 0.028
Validation Loss: 1.709
Validation Accuracy: 0.7810

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.023
Validation Loss: 1.745
Validation Accuracy: 0.7910

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.025
Validation Loss: 1.802
Validation Accuracy: 0.7690

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.019
Validation Loss: 1.928
Validation Accuracy: 0.7750


         == flag 1.601 roberta result On test data ==
# called_model : roberta
# Test Accuracy: 0.5131%
Precision: 0.7118
Recall: 0.5131
F1 Score: 0.4375
Classification Report:
              precision    recall  f1-score   support

           0       0.89      0.18      0.30      1718
           1       0.46      0.97      0.63      1252

    accuracy                           0.51      2970
   macro avg       0.68      0.58      0.46      2970
weighted avg       0.71      0.51      0.44      2970

Confusion Matrix:
[[ 309 1409]
 [  37 1215]]

flag 1.11  model:  finished  with:   roberta





===================================================== 
flag 1.10  model:  started with ==>   distilbert
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8082

Evaluating...

Training Loss: 0.609
Validation Loss: 0.564
Validation Accuracy: 0.7210

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8561

Evaluating...

Training Loss: 0.415
Validation Loss: 0.474
Validation Accuracy: 0.7660

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9067

Evaluating...

Training Loss: 0.337
Validation Loss: 0.496
Validation Accuracy: 0.7570

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9441

Evaluating...

Training Loss: 0.265
Validation Loss: 0.575
Validation Accuracy: 0.7700

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9726

Evaluating...

Training Loss: 0.203
Validation Loss: 0.585
Validation Accuracy: 0.7820

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9831

Evaluating...

Training Loss: 0.150
Validation Loss: 0.711
Validation Accuracy: 0.7740

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9907

Evaluating...

Training Loss: 0.100
Validation Loss: 0.889
Validation Accuracy: 0.7640

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9937

Evaluating...

Training Loss: 0.075
Validation Loss: 0.994
Validation Accuracy: 0.7620

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9967

Evaluating...

Training Loss: 0.054
Validation Loss: 1.123
Validation Accuracy: 0.7650

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9967

Evaluating...

Training Loss: 0.048
Validation Loss: 1.203
Validation Accuracy: 0.7610

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9979

Evaluating...

Training Loss: 0.039
Validation Loss: 1.318
Validation Accuracy: 0.7660

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.033
Validation Loss: 1.291
Validation Accuracy: 0.7730

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.030
Validation Loss: 1.436
Validation Accuracy: 0.7680

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.028
Validation Loss: 1.430
Validation Accuracy: 0.7770

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.016
Validation Loss: 1.498
Validation Accuracy: 0.7670

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.025
Validation Loss: 1.636
Validation Accuracy: 0.7560

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.028
Validation Loss: 1.534
Validation Accuracy: 0.7760

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.012
Validation Loss: 1.662
Validation Accuracy: 0.7670

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.020
Validation Loss: 1.643
Validation Accuracy: 0.7680

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.013
Validation Loss: 1.626
Validation Accuracy: 0.7700

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.020
Validation Loss: 1.704
Validation Accuracy: 0.7620

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.015
Validation Loss: 1.890
Validation Accuracy: 0.7520

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.012
Validation Loss: 1.821
Validation Accuracy: 0.7660

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.015
Validation Loss: 1.739
Validation Accuracy: 0.7710

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.011
Validation Loss: 1.804
Validation Accuracy: 0.7580

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.014
Validation Loss: 1.806
Validation Accuracy: 0.7690

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.011
Validation Loss: 1.918
Validation Accuracy: 0.7730

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.019
Validation Loss: 1.849
Validation Accuracy: 0.7750

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.015
Validation Loss: 1.865
Validation Accuracy: 0.7790

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.014
Validation Loss: 1.954
Validation Accuracy: 0.7600


         == flag 1.601 distilbert result On test data ==
# called_model : distilbert
# Test Accuracy: 0.5428%
Precision: 0.6683
Recall: 0.5428
F1 Score: 0.5016
Classification Report:
              precision    recall  f1-score   support

           0       0.81      0.28      0.41      1718
           1       0.48      0.91      0.63      1252

    accuracy                           0.54      2970
   macro avg       0.64      0.59      0.52      2970
weighted avg       0.67      0.54      0.50      2970

Confusion Matrix:
[[ 473 1245]
 [ 113 1139]]

flag 1.11  model:  finished  with:   distilbert





===================================================== 
flag 1.10  model:  started with ==>   electra
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8312

Evaluating...

Training Loss: 0.635
Validation Loss: 0.522
Validation Accuracy: 0.7410

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8810

Evaluating...

Training Loss: 0.407
Validation Loss: 0.514
Validation Accuracy: 0.7560

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9180

Evaluating...

Training Loss: 0.333
Validation Loss: 0.504
Validation Accuracy: 0.7660

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9401

Evaluating...

Training Loss: 0.274
Validation Loss: 0.588
Validation Accuracy: 0.7810

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9696

Evaluating...

Training Loss: 0.218
Validation Loss: 0.589
Validation Accuracy: 0.7900

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9746

Evaluating...

Training Loss: 0.173
Validation Loss: 0.730
Validation Accuracy: 0.7630

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9818

Evaluating...

Training Loss: 0.133
Validation Loss: 0.770
Validation Accuracy: 0.7640

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9906

Evaluating...

Training Loss: 0.106
Validation Loss: 0.895
Validation Accuracy: 0.7770

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9619

Evaluating...

Training Loss: 0.100
Validation Loss: 1.061
Validation Accuracy: 0.7690

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9916

Evaluating...

Training Loss: 0.087
Validation Loss: 0.989
Validation Accuracy: 0.7980

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9936

Evaluating...

Training Loss: 0.069
Validation Loss: 1.185
Validation Accuracy: 0.7850

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9959

Evaluating...

Training Loss: 0.070
Validation Loss: 1.042
Validation Accuracy: 0.7870

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9969

Evaluating...

Training Loss: 0.057
Validation Loss: 1.178
Validation Accuracy: 0.7770

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9916

Evaluating...

Training Loss: 0.055
Validation Loss: 1.333
Validation Accuracy: 0.7650

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9981

Evaluating...

Training Loss: 0.054
Validation Loss: 1.211
Validation Accuracy: 0.7760

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.028
Validation Loss: 1.474
Validation Accuracy: 0.7710

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.040
Validation Loss: 1.375
Validation Accuracy: 0.7760

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9983

Evaluating...

Training Loss: 0.025
Validation Loss: 1.452
Validation Accuracy: 0.7790

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.030
Validation Loss: 1.528
Validation Accuracy: 0.7820

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.032
Validation Loss: 1.531
Validation Accuracy: 0.7710

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.030
Validation Loss: 1.413
Validation Accuracy: 0.7920

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.033
Validation Loss: 1.654
Validation Accuracy: 0.7690

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.019
Validation Loss: 1.696
Validation Accuracy: 0.7660

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.022
Validation Loss: 1.687
Validation Accuracy: 0.7790

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.021
Validation Loss: 1.613
Validation Accuracy: 0.7920

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.020
Validation Loss: 1.926
Validation Accuracy: 0.7650

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.026
Validation Loss: 1.734
Validation Accuracy: 0.7760

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.014
Validation Loss: 1.789
Validation Accuracy: 0.7790

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9974

Evaluating...

Training Loss: 0.022
Validation Loss: 1.644
Validation Accuracy: 0.7810

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.025
Validation Loss: 1.552
Validation Accuracy: 0.7820


         == flag 1.601 electra result On test data ==
# called_model : electra
# Test Accuracy: 0.5138%
Precision: 0.7059
Recall: 0.5138
F1 Score: 0.4401
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.18      0.30      1718
           1       0.46      0.97      0.63      1252

    accuracy                           0.51      2970
   macro avg       0.67      0.58      0.47      2970
weighted avg       0.71      0.51      0.44      2970

Confusion Matrix:
[[ 316 1402]
 [  42 1210]]

flag 1.11  model:  finished  with:   electra





===================================================== 
flag 1.10  model:  started with ==>   gpt2
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7867

Evaluating...

Training Loss: 0.796
Validation Loss: 0.536
Validation Accuracy: 0.7180

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8200

Evaluating...

Training Loss: 0.466
Validation Loss: 0.536
Validation Accuracy: 0.7460

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8444

Evaluating...

Training Loss: 0.411
Validation Loss: 0.507
Validation Accuracy: 0.7490

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8728

Evaluating...

Training Loss: 0.375
Validation Loss: 0.487
Validation Accuracy: 0.7690

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8913

Evaluating...

Training Loss: 0.339
Validation Loss: 0.523
Validation Accuracy: 0.7590

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9200

Evaluating...

Training Loss: 0.304
Validation Loss: 0.545
Validation Accuracy: 0.7800

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9169

Evaluating...

Training Loss: 0.270
Validation Loss: 0.566
Validation Accuracy: 0.7730

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9563

Evaluating...

Training Loss: 0.222
Validation Loss: 0.648
Validation Accuracy: 0.7640

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9660

Evaluating...

Training Loss: 0.196
Validation Loss: 0.676
Validation Accuracy: 0.7720

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9800

Evaluating...

Training Loss: 0.175
Validation Loss: 0.747
Validation Accuracy: 0.7610

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9863

Evaluating...

Training Loss: 0.152
Validation Loss: 0.752
Validation Accuracy: 0.7710

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9876

Evaluating...

Training Loss: 0.123
Validation Loss: 0.875
Validation Accuracy: 0.7700

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9937

Evaluating...

Training Loss: 0.110
Validation Loss: 1.006
Validation Accuracy: 0.7700

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9957

Evaluating...

Training Loss: 0.085
Validation Loss: 1.202
Validation Accuracy: 0.7580

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9962

Evaluating...

Training Loss: 0.083
Validation Loss: 1.140
Validation Accuracy: 0.7690

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9971

Evaluating...

Training Loss: 0.083
Validation Loss: 1.202
Validation Accuracy: 0.7610

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.065
Validation Loss: 1.434
Validation Accuracy: 0.7620

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.055
Validation Loss: 1.581
Validation Accuracy: 0.7600

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9976

Evaluating...

Training Loss: 0.053
Validation Loss: 1.688
Validation Accuracy: 0.7510

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.048
Validation Loss: 1.657
Validation Accuracy: 0.7590

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.044
Validation Loss: 1.798
Validation Accuracy: 0.7570

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.039
Validation Loss: 1.869
Validation Accuracy: 0.7580

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.036
Validation Loss: 1.823
Validation Accuracy: 0.7680

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9988

Evaluating...

Training Loss: 0.038
Validation Loss: 1.798
Validation Accuracy: 0.7700

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.034
Validation Loss: 1.872
Validation Accuracy: 0.7740

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.027
Validation Loss: 2.006
Validation Accuracy: 0.7750

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.027
Validation Loss: 2.049
Validation Accuracy: 0.7750

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.027
Validation Loss: 2.153
Validation Accuracy: 0.7600

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9998

Evaluating...

Training Loss: 0.021
Validation Loss: 2.229
Validation Accuracy: 0.7780

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.020
Validation Loss: 2.205
Validation Accuracy: 0.7800


         == flag 1.601 gpt2 result On test data ==
# called_model : gpt2
# Test Accuracy: 0.4828%
Precision: 0.6956
Recall: 0.4828
F1 Score: 0.3840
Classification Report:
              precision    recall  f1-score   support

           0       0.88      0.12      0.22      1718
           1       0.45      0.98      0.61      1252

    accuracy                           0.48      2970
   macro avg       0.66      0.55      0.42      2970
weighted avg       0.70      0.48      0.38      2970

Confusion Matrix:
[[ 212 1506]
 [  30 1222]]

flag 1.11  model:  finished  with:   gpt2





===================================================== 
flag 1.10  model:  started with ==>   longformer
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8348

Evaluating...

Training Loss: 0.578
Validation Loss: 0.515
Validation Accuracy: 0.7430

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8717

Evaluating...

Training Loss: 0.393
Validation Loss: 0.572
Validation Accuracy: 0.7320

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9114

Evaluating...

Training Loss: 0.319
Validation Loss: 0.569
Validation Accuracy: 0.7500

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9473

Evaluating...

Training Loss: 0.260
Validation Loss: 0.535
Validation Accuracy: 0.7840

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9540

Evaluating...

Training Loss: 0.211
Validation Loss: 0.750
Validation Accuracy: 0.7430

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9711

Evaluating...

Training Loss: 0.173
Validation Loss: 0.839
Validation Accuracy: 0.7560

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9814

Evaluating...

Training Loss: 0.138
Validation Loss: 0.859
Validation Accuracy: 0.7700

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9912

Evaluating...

Training Loss: 0.121
Validation Loss: 0.980
Validation Accuracy: 0.7620

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9913

Evaluating...

Training Loss: 0.107
Validation Loss: 0.993
Validation Accuracy: 0.7670

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9952

Evaluating...

Training Loss: 0.090
Validation Loss: 1.002
Validation Accuracy: 0.7890

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9944

Evaluating...

Training Loss: 0.081
Validation Loss: 1.142
Validation Accuracy: 0.7660

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9970

Evaluating...

Training Loss: 0.073
Validation Loss: 1.264
Validation Accuracy: 0.7860

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9974

Evaluating...

Training Loss: 0.062
Validation Loss: 1.314
Validation Accuracy: 0.7750

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.057
Validation Loss: 1.363
Validation Accuracy: 0.7820

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9979

Evaluating...

Training Loss: 0.056
Validation Loss: 1.490
Validation Accuracy: 0.7700

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9980

Evaluating...

Training Loss: 0.052
Validation Loss: 1.308
Validation Accuracy: 0.7850

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9982

Evaluating...

Training Loss: 0.048
Validation Loss: 1.466
Validation Accuracy: 0.7790

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.054
Validation Loss: 1.483
Validation Accuracy: 0.7650

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.046
Validation Loss: 1.609
Validation Accuracy: 0.7840

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.034
Validation Loss: 1.835
Validation Accuracy: 0.7610

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.037
Validation Loss: 1.744
Validation Accuracy: 0.7670

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9977

Evaluating...

Training Loss: 0.033
Validation Loss: 1.907
Validation Accuracy: 0.7530

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.034
Validation Loss: 1.755
Validation Accuracy: 0.7700

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.028
Validation Loss: 1.868
Validation Accuracy: 0.7610

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.027
Validation Loss: 1.839
Validation Accuracy: 0.7680

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.023
Validation Loss: 1.669
Validation Accuracy: 0.7820

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9997

Evaluating...

Training Loss: 0.031
Validation Loss: 1.745
Validation Accuracy: 0.7820

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.042
Validation Loss: 1.793
Validation Accuracy: 0.7730

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.017
Validation Loss: 1.904
Validation Accuracy: 0.7820

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.029
Validation Loss: 1.856
Validation Accuracy: 0.7770


         == flag 1.601 longformer result On test data ==
# called_model : longformer
# Test Accuracy: 0.4896%
Precision: 0.6807
Recall: 0.4896
F1 Score: 0.4008
Classification Report:
              precision    recall  f1-score   support

           0       0.85      0.14      0.25      1718
           1       0.45      0.96      0.61      1252

    accuracy                           0.49      2970
   macro avg       0.65      0.55      0.43      2970
weighted avg       0.68      0.49      0.40      2970

Confusion Matrix:
[[ 246 1472]
 [  44 1208]]

flag 1.11  model:  finished  with:   longformer





===================================================== 
flag 1.10  model:  started with ==>   luke
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8164

Evaluating...

Training Loss: 0.612
Validation Loss: 0.578
Validation Accuracy: 0.7330

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8678

Evaluating...

Training Loss: 0.399
Validation Loss: 0.498
Validation Accuracy: 0.7600

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9102

Evaluating...

Training Loss: 0.337
Validation Loss: 0.503
Validation Accuracy: 0.7740

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9467

Evaluating...

Training Loss: 0.274
Validation Loss: 0.530
Validation Accuracy: 0.7880

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9616

Evaluating...

Training Loss: 0.217
Validation Loss: 0.572
Validation Accuracy: 0.7740

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9751

Evaluating...

Training Loss: 0.169
Validation Loss: 0.792
Validation Accuracy: 0.7610

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9861

Evaluating...

Training Loss: 0.141
Validation Loss: 0.871
Validation Accuracy: 0.7700

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9909

Evaluating...

Training Loss: 0.125
Validation Loss: 0.919
Validation Accuracy: 0.7740

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9920

Evaluating...

Training Loss: 0.103
Validation Loss: 1.152
Validation Accuracy: 0.7630

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9942

Evaluating...

Training Loss: 0.097
Validation Loss: 1.253
Validation Accuracy: 0.7680

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9932

Evaluating...

Training Loss: 0.081
Validation Loss: 1.285
Validation Accuracy: 0.7590

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9966

Evaluating...

Training Loss: 0.072
Validation Loss: 1.264
Validation Accuracy: 0.7830

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9974

Evaluating...

Training Loss: 0.068
Validation Loss: 1.455
Validation Accuracy: 0.7690

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9976

Evaluating...

Training Loss: 0.057
Validation Loss: 1.344
Validation Accuracy: 0.7830

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9978

Evaluating...

Training Loss: 0.061
Validation Loss: 1.579
Validation Accuracy: 0.7720

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.056
Validation Loss: 1.453
Validation Accuracy: 0.7720

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.046
Validation Loss: 1.448
Validation Accuracy: 0.7690

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.040
Validation Loss: 1.615
Validation Accuracy: 0.7670

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.035
Validation Loss: 1.690
Validation Accuracy: 0.7690

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.040
Validation Loss: 1.606
Validation Accuracy: 0.7720

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.040
Validation Loss: 1.801
Validation Accuracy: 0.7650

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.027
Validation Loss: 1.633
Validation Accuracy: 0.7770

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.038
Validation Loss: 1.680
Validation Accuracy: 0.7780

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9984

Evaluating...

Training Loss: 0.038
Validation Loss: 1.661
Validation Accuracy: 0.7850

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.028
Validation Loss: 1.702
Validation Accuracy: 0.7750

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.027
Validation Loss: 1.618
Validation Accuracy: 0.7850

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.021
Validation Loss: 1.597
Validation Accuracy: 0.7850

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.028
Validation Loss: 1.679
Validation Accuracy: 0.7880

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.029
Validation Loss: 1.658
Validation Accuracy: 0.7940

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.021
Validation Loss: 1.922
Validation Accuracy: 0.7790


         == flag 1.601 luke result On test data ==
# called_model : luke
# Test Accuracy: 0.5165%
Precision: 0.6580
Recall: 0.5165
F1 Score: 0.4590
Classification Report:
              precision    recall  f1-score   support

           0       0.80      0.22      0.34      1718
           1       0.46      0.92      0.62      1252

    accuracy                           0.52      2970
   macro avg       0.63      0.57      0.48      2970
weighted avg       0.66      0.52      0.46      2970

Confusion Matrix:
[[ 376 1342]
 [  94 1158]]

flag 1.11  model:  finished  with:   luke





===================================================== 
flag 1.10  model:  started with ==>   t5
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.953
Validation Loss: 0.706
Validation Accuracy: 0.5730

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.5797

Evaluating...

Training Loss: 0.705
Validation Loss: 0.665
Validation Accuracy: 0.5730

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7356

Evaluating...

Training Loss: 0.637
Validation Loss: 0.584
Validation Accuracy: 0.6960

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.7812

Evaluating...

Training Loss: 0.553
Validation Loss: 0.553
Validation Accuracy: 0.7290

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8089

Evaluating...

Training Loss: 0.502
Validation Loss: 0.548
Validation Accuracy: 0.7360

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8242

Evaluating...

Training Loss: 0.465
Validation Loss: 0.563
Validation Accuracy: 0.7510

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8326

Evaluating...

Training Loss: 0.440
Validation Loss: 0.566
Validation Accuracy: 0.7360

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8368

Evaluating...

Training Loss: 0.420
Validation Loss: 0.583
Validation Accuracy: 0.7340

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8588

Evaluating...

Training Loss: 0.398
Validation Loss: 0.573
Validation Accuracy: 0.7430

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8673

Evaluating...

Training Loss: 0.384
Validation Loss: 0.591
Validation Accuracy: 0.7330

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8753

Evaluating...

Training Loss: 0.372
Validation Loss: 0.602
Validation Accuracy: 0.7440

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8849

Evaluating...

Training Loss: 0.360
Validation Loss: 0.588
Validation Accuracy: 0.7500

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8836

Evaluating...

Training Loss: 0.344
Validation Loss: 0.620
Validation Accuracy: 0.7400

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9006

Evaluating...

Training Loss: 0.330
Validation Loss: 0.629
Validation Accuracy: 0.7380

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9083

Evaluating...

Training Loss: 0.321
Validation Loss: 0.629
Validation Accuracy: 0.7380

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9159

Evaluating...

Training Loss: 0.309
Validation Loss: 0.632
Validation Accuracy: 0.7420

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9247

Evaluating...

Training Loss: 0.298
Validation Loss: 0.636
Validation Accuracy: 0.7490

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9248

Evaluating...

Training Loss: 0.283
Validation Loss: 0.681
Validation Accuracy: 0.7490

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9361

Evaluating...

Training Loss: 0.271
Validation Loss: 0.687
Validation Accuracy: 0.7490

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9449

Evaluating...

Training Loss: 0.251
Validation Loss: 0.701
Validation Accuracy: 0.7550

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9507

Evaluating...

Training Loss: 0.250
Validation Loss: 0.730
Validation Accuracy: 0.7450

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9553

Evaluating...

Training Loss: 0.236
Validation Loss: 0.774
Validation Accuracy: 0.7520

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9594

Evaluating...

Training Loss: 0.216
Validation Loss: 0.797
Validation Accuracy: 0.7530

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9641

Evaluating...

Training Loss: 0.207
Validation Loss: 0.844
Validation Accuracy: 0.7460

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9659

Evaluating...

Training Loss: 0.209
Validation Loss: 0.846
Validation Accuracy: 0.7410

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9710

Evaluating...

Training Loss: 0.190
Validation Loss: 0.888
Validation Accuracy: 0.7440

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9744

Evaluating...

Training Loss: 0.185
Validation Loss: 0.879
Validation Accuracy: 0.7420

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9802

Evaluating...

Training Loss: 0.181
Validation Loss: 0.906
Validation Accuracy: 0.7480

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9769

Evaluating...

Training Loss: 0.163
Validation Loss: 1.003
Validation Accuracy: 0.7470

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9843

Evaluating...

Training Loss: 0.167
Validation Loss: 0.927
Validation Accuracy: 0.7590


         == flag 1.601 t5 result On test data ==
# called_model : t5
# Test Accuracy: 0.4838%
Precision: 0.6465
Recall: 0.4838
F1 Score: 0.3988
Classification Report:
              precision    recall  f1-score   support

           0       0.79      0.15      0.25      1718
           1       0.45      0.95      0.61      1252

    accuracy                           0.48      2970
   macro avg       0.62      0.55      0.43      2970
weighted avg       0.65      0.48      0.40      2970

Confusion Matrix:
[[ 251 1467]
 [  66 1186]]

flag 1.11  model:  finished  with:   t5





===================================================== 
flag 1.10  model:  started with ==>   xlnet
===================================================== 

 Epoch 1 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8081

Evaluating...

Training Loss: 0.595
Validation Loss: 0.526
Validation Accuracy: 0.7290

 Epoch 2 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8627

Evaluating...

Training Loss: 0.442
Validation Loss: 0.535
Validation Accuracy: 0.7450

 Epoch 3 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.8951

Evaluating...

Training Loss: 0.366
Validation Loss: 0.523
Validation Accuracy: 0.7610

 Epoch 4 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9369

Evaluating...

Training Loss: 0.306
Validation Loss: 0.570
Validation Accuracy: 0.7640

 Epoch 5 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9629

Evaluating...

Training Loss: 0.246
Validation Loss: 0.628
Validation Accuracy: 0.7570

 Epoch 6 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9784

Evaluating...

Training Loss: 0.194
Validation Loss: 0.807
Validation Accuracy: 0.7600

 Epoch 7 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9887

Evaluating...

Training Loss: 0.145
Validation Loss: 0.756
Validation Accuracy: 0.7770

 Epoch 8 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9932

Evaluating...

Training Loss: 0.114
Validation Loss: 0.976
Validation Accuracy: 0.7530

 Epoch 9 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9953

Evaluating...

Training Loss: 0.085
Validation Loss: 1.180
Validation Accuracy: 0.7690

 Epoch 10 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9943

Evaluating...

Training Loss: 0.082
Validation Loss: 1.177
Validation Accuracy: 0.7680

 Epoch 11 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9953

Evaluating...

Training Loss: 0.065
Validation Loss: 1.662
Validation Accuracy: 0.7520

 Epoch 12 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9973

Evaluating...

Training Loss: 0.059
Validation Loss: 1.629
Validation Accuracy: 0.7590

 Epoch 13 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9936

Evaluating...

Training Loss: 0.064
Validation Loss: 1.769
Validation Accuracy: 0.7460

 Epoch 14 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9987

Evaluating...

Training Loss: 0.054
Validation Loss: 1.684
Validation Accuracy: 0.7680

 Epoch 15 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9950

Evaluating...

Training Loss: 0.037
Validation Loss: 2.147
Validation Accuracy: 0.7450

 Epoch 16 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9989

Evaluating...

Training Loss: 0.048
Validation Loss: 1.758
Validation Accuracy: 0.7770

 Epoch 17 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9986

Evaluating...

Training Loss: 0.034
Validation Loss: 2.064
Validation Accuracy: 0.7640

 Epoch 18 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9981

Evaluating...

Training Loss: 0.041
Validation Loss: 1.732
Validation Accuracy: 0.7740

 Epoch 19 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9990

Evaluating...

Training Loss: 0.034
Validation Loss: 2.046
Validation Accuracy: 0.7670

 Epoch 20 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.035
Validation Loss: 1.825
Validation Accuracy: 0.7710

 Epoch 21 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.023
Validation Loss: 2.061
Validation Accuracy: 0.7640

 Epoch 22 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.031
Validation Loss: 2.012
Validation Accuracy: 0.7700

 Epoch 23 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.015
Validation Loss: 2.296
Validation Accuracy: 0.7730

 Epoch 24 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.027
Validation Loss: 2.119
Validation Accuracy: 0.7760

 Epoch 25 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9994

Evaluating...

Training Loss: 0.022
Validation Loss: 2.076
Validation Accuracy: 0.7730

 Epoch 26 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.030
Validation Loss: 1.938
Validation Accuracy: 0.7810

 Epoch 27 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9991

Evaluating...

Training Loss: 0.026
Validation Loss: 2.015
Validation Accuracy: 0.7710

 Epoch 28 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9993

Evaluating...

Training Loss: 0.023
Validation Loss: 2.214
Validation Accuracy: 0.7590

 Epoch 29 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9996

Evaluating...

Training Loss: 0.025
Validation Loss: 1.919
Validation Accuracy: 0.7790

 Epoch 30 / 30
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.

Evaluating...
  Batch    50  of    282.
  Batch   100  of    282.
  Batch   150  of    282.
  Batch   200  of    282.
  Batch   250  of    282.
Training Accuracy: 0.9992

Evaluating...

Training Loss: 0.018
Validation Loss: 2.199
Validation Accuracy: 0.7700


         == flag 1.601 xlnet result On test data ==
# called_model : xlnet
# Test Accuracy: 0.5020%
Precision: 0.6775
Recall: 0.5020
F1 Score: 0.4258
Classification Report:
              precision    recall  f1-score   support

           0       0.84      0.17      0.29      1718
           1       0.46      0.95      0.62      1252

    accuracy                           0.50      2970
   macro avg       0.65      0.56      0.45      2970
weighted avg       0.68      0.50      0.43      2970

Confusion Matrix:
[[ 296 1422]
 [  57 1195]]

flag 1.11  model:  finished  with:   xlnet
