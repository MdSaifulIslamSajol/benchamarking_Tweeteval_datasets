learning rate  : 1e-06
epochs : 2
                                                text  label
0  “Worry is a down payment on a problem you may ...      2
1  My roommate: it's okay that we can't spell bec...      0
2  No but that's so cute. Atsu was probably shy a...      1
3  Rooneys fucking untouchable isn't he? Been fuc...      0
4  it's pretty depressing when u hit pan on ur fa...      3
                                                text
0  “Worry is a down payment on a problem you may ...
1  My roommate: it's okay that we can't spell bec...
2  No but that's so cute. Atsu was probably shy a...
3  Rooneys fucking untouchable isn't he? Been fuc...
4  it's pretty depressing when u hit pan on ur fa...
   label
0      2
1      0
2      1
3      0
4      3
                                                text  label
0  @user @user Oh, hidden revenge and anger...I r...      0
1  if not then #teamchristine bc all tana has don...      0
2  Hey @user #Fields in #skibbereen give your onl...      0
3  Why have #Emmerdale had to rob #robron of havi...      0
4  @user I would like to hear a podcast of you go...      0
                                                text
0  @user @user Oh, hidden revenge and anger...I r...
1  if not then #teamchristine bc all tana has don...
2  Hey @user #Fields in #skibbereen give your onl...
3  Why have #Emmerdale had to rob #robron of havi...
4  @user I would like to hear a podcast of you go...
   label
0      0
1      0
2      0
3      0
4      0
                                                text  label
0  #Deppression is real. Partners w/ #depressed p...      3
1  @user Interesting choice of words... Are you c...      0
2  My visit to hospital for care triggered #traum...      3
3  @user Welcome to #MPSVT! We are delighted to h...      1
4                       What makes you feel #joyful?      1
                                                text
0  #Deppression is real. Partners w/ #depressed p...
1  @user Interesting choice of words... Are you c...
2  My visit to hospital for care triggered #traum...
3  @user Welcome to #MPSVT! We are delighted to h...
4                       What makes you feel #joyful?
                                                text
0  #Deppression is real. Partners w/ #depressed p...
1  @user Interesting choice of words... Are you c...
2  My visit to hospital for care triggered #traum...
3  @user Welcome to #MPSVT! We are delighted to h...
4                       What makes you feel #joyful?
len(train_labels) 3257
len(test_labels) 1421
len(val_labels) 374

Unique values count in train_labels:
label
0    1400
3     855
1     708
2     294
Name: count, dtype: int64

Unique values count in val_labels:
label
0    160
1     97
3     89
2     28
Name: count, dtype: int64

Unique values count in test_labels:
label
0    558
3    382
1    358
2    123
Name: count, dtype: int64





===================================================== 
flag 1.10  model:  started with ==>   bert
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4200

Evaluating...

Training Loss: 1.371
Validation Loss: 1.325
Validation Accuracy: 0.4091

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4412

Evaluating...

Training Loss: 1.306
Validation Loss: 1.268
Validation Accuracy: 0.4225


         == flag 1.601 bert result On test data ==
# called_model : bert
# Test Accuracy: 0.4018%
Precision: 0.3119
Recall: 0.4018
F1 Score: 0.2506
Classification Report:
              precision    recall  f1-score   support

           0       0.40      0.98      0.57       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.58      0.06      0.10       382

    accuracy                           0.40      1421
   macro avg       0.24      0.26      0.17      1421
weighted avg       0.31      0.40      0.25      1421

Confusion Matrix:
[[549   0   0   9]
 [356   0   0   2]
 [118   0   0   5]
 [357   3   0  22]]

flag 1.11  model:  finished  with:   bert





===================================================== 
flag 1.10  model:  started with ==>   roberta
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4298

Evaluating...

Training Loss: 1.355
Validation Loss: 1.333
Validation Accuracy: 0.4278

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4298

Evaluating...

Training Loss: 1.307
Validation Loss: 1.258
Validation Accuracy: 0.4278


         == flag 1.601 roberta result On test data ==
# called_model : roberta
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   roberta





===================================================== 
flag 1.10  model:  started with ==>   distilbert
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4305

Evaluating...

Training Loss: 1.347
Validation Loss: 1.304
Validation Accuracy: 0.4305

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4298

Evaluating...

Training Loss: 1.276
Validation Loss: 1.238
Validation Accuracy: 0.4278


         == flag 1.601 distilbert result On test data ==
# called_model : distilbert
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   distilbert





===================================================== 
flag 1.10  model:  started with ==>   electra
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4295

Evaluating...

Training Loss: 1.345
Validation Loss: 1.304
Validation Accuracy: 0.4251

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4298

Evaluating...

Training Loss: 1.294
Validation Loss: 1.265
Validation Accuracy: 0.4278


         == flag 1.601 electra result On test data ==
# called_model : electra
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   electra





===================================================== 
flag 1.10  model:  started with ==>   gpt2
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.3436

Evaluating...

Training Loss: 1.611
Validation Loss: 1.303
Validation Accuracy: 0.3583

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...
  Batch    50  of    102.
  Batch   100  of    102.
Training Accuracy: 0.4034

Evaluating...

Training Loss: 1.381
Validation Loss: 1.280
Validation Accuracy: 0.4171


         == flag 1.601 gpt2 result On test data ==
# called_model : gpt2
# Test Accuracy: 0.3702%
Precision: 0.4683
Recall: 0.3702
F1 Score: 0.2452
Classification Report:
              precision    recall  f1-score   support

           0       0.39      0.89      0.55       558
           1       0.18      0.08      0.11       358
           2       0.00      0.00      0.00       123
           3       1.00      0.01      0.01       382

    accuracy                           0.37      1421
   macro avg       0.39      0.24      0.17      1421
weighted avg       0.47      0.37      0.25      1421

Confusion Matrix:
[[495  63   0   0]
 [329  29   0   0]
 [111  12   0   0]
 [320  60   0   2]]

flag 1.11  model:  finished  with:   gpt2





===================================================== 
flag 1.10  model:  started with ==>   longformer
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.
