#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Created on Fri May 24 21:47:31 2024

@author: saiful
"""

# -*- coding: utf-8 -*-
"""x1_FakeNewDetection.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1KPShsnIudbmI_y-brdDEcdPYLQSbtZqQ

## Setup Environment
"""
# https://github.com/cardiffnlp/tweeteval
# https://github.com/Vicomtech/hate-speech-dataset
# https://www.kaggle.com/datasets/thedevastator/tweeteval-a-multi-task-classification-benchmark?select=emotion_train.csv
import numpy as np
import pandas as pd
import transformers
from transformers import AutoModel, BertTokenizerFast
from transformers import  DistilBertModel, DistilBertTokenizerFast
import matplotlib.pyplot as plt
from sklearn.metrics import ConfusionMatrixDisplay
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import torch
import torch.nn as nn
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

"""## Load Dataset"""

# Load the text file using pandas
emotion_train = pd.read_csv("/home/saiful/bangla fault news/tweeteval/tweeteval kaggle/emotion_train.csv")
# Display the first few rows of the dataframe
print(emotion_train.head())

# Separate the dataframe into two dataframes
train_text = emotion_train[['text']].copy()  # Retain the 'text' column with the header
train_labels = emotion_train[['label']].copy()  # Retain the 'label' column with the header

# Display the first few rows of each dataframe
print(train_text.head())
print(train_labels.head())

# val
# Load the text file using pandas
emotion_validation = pd.read_csv("/home/saiful/bangla fault news/tweeteval/tweeteval kaggle/emotion_validation.csv")
# Display the first few rows of the dataframe
print(emotion_validation.head())

# Separate the dataframe into two dataframes
val_text = emotion_validation[['text']].copy()  # Retain the 'text' column with the header
val_labels = emotion_validation[['label']].copy()  # Retain the 'label' column with the header

# Display the first few rows of each dataframe
print(val_text.head())
print(val_labels.head())

## ## test
# Load the text file using pandas
emotion_test = pd.read_csv("/home/saiful/bangla fault news/tweeteval/tweeteval kaggle/emotion_test.csv")
# Display the first few rows of the dataframe
print(emotion_test.head())

# Separate the dataframe into two dataframes
test_text = emotion_test[['text']].copy()  # Retain the 'text' column with the header
test_labels = emotion_test[['label']].copy()  # Retain the 'label' column with the header

# Display the first few rows of each dataframe
print(test_text.head())
print(test_text.head())


train_text =train_text.squeeze()
val_text=val_text.squeeze()
test_text=test_text.squeeze()

train_labels =train_labels.squeeze()
val_labels =val_labels.squeeze()
test_labels =test_labels.squeeze()

# Convert each item inside the series to a string
train_text = train_text.apply(str)
val_text = val_text.apply(str)
test_text = test_text.apply(str)

#%%
# # Checking if our data is well balanced
# label_size = [data['label'].sum(), len(data['label']) - data['label'].sum()]
# plt.pie(label_size, explode=[0.1, 0.1], colors=['firebrick', 'navy'], startangle=90, shadow=True, labels=['Fake', 'True'], autopct='%1.1f%%')

# """## Train-test-split"""

# # Train-Validation-Test set split into 70:15:15 ratio
# # Train-Temp split
# train_text, temp_text, train_labels, temp_labels = train_test_split(data['text'], data['label'],
#                                                                     random_state=2018,
#                                                                     test_size=0.3,
#                                                                     stratify=data['label'])
# # Validation-Test split
# val_text, test_text, val_labels, test_labels = train_test_split(temp_text, temp_labels,
#                                                                 random_state=2018,
#                                                                 test_size=0.5,
#                                                                 stratify=temp_labels)
#%% model
"""## BERT Fine-tuning

### Load pretrained BERT Model
"""
from transformers import AutoModel, AutoTokenizer
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from transformers import GPT2Model, GPT2TokenizerFast, GPT2ForSequenceClassification
from transformers import LongformerModel, LongformerTokenizerFast
from transformers import LukeModel, LukeTokenizer
from transformers import T5Model, T5Tokenizer
from transformers import XLNetModel, XLNetTokenizer
from transformers import ElectraModel, ElectraTokenizer


# # # # # Load BERT model and tokenizer via HuggingFace Transformers
# model = AutoModel.from_pretrained('bert-base-uncased')
# tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')


# # # Load the RoBERTa model and tokenizer
# model = AutoModel.from_pretrained("roberta-base")
# tokenizer = AutoTokenizer.from_pretrained("roberta-base")


# # Load the DistilBERT model and tokenizer
# model = AutoModel.from_pretrained("distilbert-base-uncased")
# tokenizer = AutoTokenizer.from_pretrained("distilbert-base-uncased")


# # Load the DistilBERT model and tokenizer
# model = DistilBertModel.from_pretrained("distilbert-base-uncased")
# tokenizer = DistilBertTokenizerFast.from_pretrained("distilbert-base-uncased")



# Load the ELECTRA model and tokenizer
model_name = "google/electra-base-discriminator"
tokenizer = ElectraTokenizer.from_pretrained(model_name)
model = ElectraModel.from_pretrained(model_name)

# # Load the GPT-2 model and tokenizer
# model_name = "gpt2"
# tokenizer = GPT2TokenizerFast.from_pretrained(model_name)
# model = GPT2Model.from_pretrained(model_name)
# # Add padding token to tokenizer and resize model embeddings
# tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model.resize_token_embeddings(len(tokenizer))


# # Load the Longformer model and tokenizer
# model_name = "allenai/longformer-base-4096"
# tokenizer = LongformerTokenizerFast.from_pretrained(model_name)
# model = LongformerModel.from_pretrained(model_name)


# # Load the LUKE model and tokenizer
# model_name = "studio-ousia/luke-base"
# tokenizer = LukeTokenizer.from_pretrained(model_name)
# model = LukeModel.from_pretrained(model_name)


# # Load the T5 model and tokenizer
# model_name = "t5-base"
# tokenizer = T5Tokenizer.from_pretrained(model_name)
# model = T5Model.from_pretrained(model_name)
# # Add padding token to tokenizer
# tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model.resize_token_embeddings(len(tokenizer))
# MAX_LENGTH = 100  # T5's max length


# # Load the XLNet model and tokenizer
# model_name = "xlnet-base-cased"
# tokenizer = XLNetTokenizer.from_pretrained(model_name)
# model = XLNetModel.from_pretrained(model_name)
# # Add padding token to tokenizer
# tokenizer.add_special_tokens({'pad_token': '[PAD]'})
# model.resize_token_embeddings(len(tokenizer))

"""### Prepare Input Data"""
#plot
# # Plot histogram of the number of words in train data 'title'
# seq_len = [len(title.split()) for title in train_text]

# pd.Series(seq_len).hist(bins=40, color='firebrick')
# plt.xlabel('Number of Words')
# plt.ylabel('Number of texts')


#%
# BERT Tokeizer Functionality
sample_data = ["Build fake news model.",
               "Using bert."]  # sample data
tokenized_sample_data = tokenizer.batch_encode_plus(sample_data, padding=True)  # encode text
print(tokenized_sample_data)

# Majority of titles above have word length under 15. So, we set max title length as 15
MAX_LENGTH = 100
# Tokenize and encode sequences in the train set
tokens_train = tokenizer.batch_encode_plus(
    train_text.tolist(),
    max_length=MAX_LENGTH,
    pad_to_max_length=True,
    truncation=True
)
# tokenize and encode sequences in the validation set
tokens_val = tokenizer.batch_encode_plus(
    val_text.tolist(),
    max_length=MAX_LENGTH,
    pad_to_max_length=True,
    truncation=True
)
# tokenize and encode sequences in the test set
tokens_test = tokenizer.batch_encode_plus(
    test_text.tolist(),
    max_length=MAX_LENGTH,
    pad_to_max_length=True,
    truncation=True
)

# Convert lists to tensors
train_seq = torch.tensor(tokens_train['input_ids']).to(device)
train_mask = torch.tensor(tokens_train['attention_mask']).to(device)
train_y = torch.tensor(train_labels.tolist()).to(device)

val_seq = torch.tensor(tokens_val['input_ids']).to(device)
val_mask = torch.tensor(tokens_val['attention_mask']).to(device)
val_y = torch.tensor(val_labels.tolist()).to(device)

test_seq = torch.tensor(tokens_test['input_ids']).to(device)
test_mask = torch.tensor(tokens_test['attention_mask']).to(device)
test_y = torch.tensor(test_labels.tolist()).to(device)

# Data Loader structure definition
from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler
batch_size = 32  # define a batch size

train_data = TensorDataset(train_seq, train_mask, train_y)  # wrap tensors
train_sampler = RandomSampler(train_data)  # sampler for sampling the data during training
train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)  # dataLoader for train set
val_data = TensorDataset(val_seq, val_mask, val_y)  # wrap tensors
val_sampler = SequentialSampler(val_data)  # sampler for sampling the data during training
val_dataloader = DataLoader(val_data, sampler=val_sampler, batch_size=batch_size)  # dataLoader for validation set

"""### Freeze Layers"""

# Freezing the parameters and defining trainable BERT structure
for param in model.parameters():
    param.requires_grad = True  # false here means gradient need not be computed

"""### Define Model Architecture"""

class BERT_Arch(nn.Module):
    def __init__(self, bert):
        super(BERT_Arch, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        cls_hs = self.bert(sent_id, attention_mask=mask)['pooler_output']  # pass the inputs to the model
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x


# model = BERT_Arch(model).to(device)

# Define the DistilBERT architecture
class DistilBERT_Arch(nn.Module):
    def __init__(self, bert):
        super(DistilBERT_Arch, self).__init__()
        self.bert = bert
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        cls_hs = self.bert(input_ids=sent_id, attention_mask=mask).last_hidden_state[:, 0, :]  # use the hidden state of the [CLS] token
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

# model = DistilBERT_Arch(model).to(device)


# Define the GPT2 architecture for sequence classification
class GPT2_Arch(nn.Module):
    def __init__(self, gpt_model):
        super(GPT2_Arch, self).__init__()
        self.gpt = gpt_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        gpt_output = self.gpt(input_ids=sent_id, attention_mask=mask)
        cls_hs = gpt_output.last_hidden_state[:, -1, :]  # use the hidden state of the last token
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x
# model = GPT2_Arch(model).to(device)

# Define the Longformer architecture for sequence classification
class Longformer_Arch(nn.Module):
    def __init__(self, longformer_model):
        super(Longformer_Arch, self).__init__()
        self.longformer = longformer_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        longformer_output = self.longformer(input_ids=sent_id, attention_mask=mask)
        cls_hs = longformer_output.last_hidden_state[:, 0, :]  # use the hidden state of the first token ([CLS] token)
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

# model = Longformer_Arch(model).to(device)

# Define the LUKE architecture for sequence classification
class LUKE_Arch(nn.Module):
    def __init__(self, luke_model):
        super(LUKE_Arch, self).__init__()
        self.luke = luke_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        luke_output = self.luke(input_ids=sent_id, attention_mask=mask)
        cls_hs = luke_output.last_hidden_state[:, 0, :]  # use the hidden state of the first token ([CLS] token)
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

# model = LUKE_Arch(model).to(device)

# Define the T5 architecture for sequence classification
class T5_Arch(nn.Module):
    def __init__(self, t5_model):
        super(T5_Arch, self).__init__()
        self.t5 = t5_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        decoder_input_ids = torch.zeros(sent_id.shape, dtype=torch.long).to(device)
        t5_output = self.t5(input_ids=sent_id, attention_mask=mask, decoder_input_ids=decoder_input_ids)
        cls_hs = t5_output.last_hidden_state[:, 0, :]  # use the hidden state of the first token ([CLS] token)
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

# model = T5_Arch(model).to(device)

# Define the XLNet architecture for sequence classification
class XLNet_Arch(nn.Module):
    def __init__(self, xlnet_model):
        super(XLNet_Arch, self).__init__()
        self.xlnet = xlnet_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        xlnet_output = self.xlnet(input_ids=sent_id, attention_mask=mask)
        cls_hs = xlnet_output.last_hidden_state[:, -1, :]  # use the hidden state of the last token
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

# model = XLNet_Arch(model).to(device)

# Define the ELECTRA architecture for sequence classification
class ELECTRA_Arch(nn.Module):
    def __init__(self, electra_model):
        super(ELECTRA_Arch, self).__init__()
        self.electra = electra_model
        self.dropout = nn.Dropout(0.1)  # dropout layer
        self.relu = nn.ReLU()  # relu activation function
        self.fc1 = nn.Linear(768, 512)  # dense layer 1
        self.fc2 = nn.Linear(512, 4)  # dense layer 2 (Output layer)
        self.softmax = nn.LogSoftmax(dim=1)  # softmax activation function

    def forward(self, sent_id, mask):  # define the forward pass
        electra_output = self.electra(input_ids=sent_id, attention_mask=mask)
        cls_hs = electra_output.last_hidden_state[:, 0, :]  # use the hidden state of the first token ([CLS] token)
        x = self.fc1(cls_hs)
        x = self.relu(x)
        x = self.dropout(x)
        x = self.fc2(x)  # output layer
        x = self.softmax(x)  # apply softmax activation
        return x

model = ELECTRA_Arch(model).to(device)

# Defining the hyperparameters (optimizer, weights of the classes and the epochs)
# from transformers import AdamW, Adam
from torch.optim import Adam, AdamW
optimizer = AdamW(model.parameters(), lr=1e-4)  # learning rate
# cross_entropy = nn.NLLLoss()  # Define the loss function
cross_entropy = nn.CrossEntropyLoss()  # Define the loss function

epochs = 3  # Number of training epochs

#%%
"""### Define Train & Evaluate Function"""

# Defining training and evaluation functions
def train():
    model.train()
    total_loss, total_accuracy = 0, 0

    for step, batch in enumerate(train_dataloader):  # iterate over batches
        if step % 50 == 0 and not step == 0:  # progress update after every 50 batches.
            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(train_dataloader)))
        batch = [r.to(device) for r in batch]  # push the batch to gpu
        sent_id, mask, labels = batch
        model.zero_grad()  # clear previously calculated gradients
        preds = model(sent_id, mask)  # get model predictions for current batch
        loss = cross_entropy(preds, labels.long())  # compute loss between actual & predicted values
        total_loss = total_loss + loss.item()  # add on to the total loss
        loss.backward()  # backward pass to calculate the gradients
        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)  # clip gradients to 1.0. It helps in preventing exploding gradient problem
        optimizer.step()  # update parameters

    avg_loss = total_loss / len(train_dataloader)  # compute training loss of the epoch
    return avg_loss  # returns the loss

def evaluate():
    print("\nEvaluating...")
    model.eval()  # Deactivate dropout layers
    total_loss, total_accuracy = 0, 0
    total_preds, total_labels = [], []

    for step, batch in enumerate(val_dataloader):  # Iterate over batches
        if step % 50 == 0 and not step == 0:  # Progress update every 50 batches.
            print('  Batch {:>5,}  of  {:>5,}.'.format(step, len(val_dataloader)))

        batch = [t.to(device) for t in batch]  # Push the batch to GPU
        sent_id, mask, labels = batch
        with torch.no_grad():  # Deactivate autograd
            preds = model(sent_id, mask)  # Model predictions
            loss = cross_entropy(preds, labels.long())  # Compute the validation loss between actual and predicted values
            total_loss += loss.item()

            preds = preds.detach().cpu().numpy()
            labels = labels.detach().cpu().numpy()
            total_preds.extend(np.argmax(preds, axis=1))
            total_labels.extend(labels)

    avg_loss = total_loss / len(val_dataloader)  # Compute the validation loss of the epoch
    total_accuracy = np.sum(np.array(total_preds) == np.array(total_labels)) / len(total_labels)  # Compute accuracy

    return avg_loss, total_accuracy

# Train and predict
best_valid_loss = float('inf')
train_losses = []  # Empty lists to store training and validation loss of each epoch
valid_losses = []
valid_accuracies = []

for epoch in range(epochs):
    print('\n Epoch {:} / {:}'.format(epoch + 1, epochs))
    # Train model
    train_loss = train()
    # Evaluate model
    valid_loss, valid_accuracy = evaluate()
    
    if valid_loss < best_valid_loss:  # Save the best model
        best_valid_loss = valid_loss
        torch.save(model.state_dict(), 'c2_new_model_weights.pt')

    train_losses.append(train_loss)  # Append training and validation loss
    valid_losses.append(valid_loss)
    valid_accuracies.append(valid_accuracy)

    print(f'\nTraining Loss: {train_loss:.3f}')
    print(f'Validation Loss: {valid_loss:.3f}')
    print(f'Validation Accuracy: {valid_accuracy:.3f}')

# Load weights of the best model
path = 'c2_new_model_weights.pt'
model.load_state_dict(torch.load(path))

# Evaluate the model on the test set
with torch.no_grad():
    preds = model(test_seq, test_mask)
    preds = preds.detach().cpu().numpy()

preds = np.argmax(preds, axis=1)
print(classification_report(test_y.cpu(), preds))
