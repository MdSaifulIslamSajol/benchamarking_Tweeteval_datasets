learning rate  : 1e-06
epochs : 2
                                                text  label
0  “Worry is a down payment on a problem you may ...      2
1  My roommate: it's okay that we can't spell bec...      0
2  No but that's so cute. Atsu was probably shy a...      1
3  Rooneys fucking untouchable isn't he? Been fuc...      0
4  it's pretty depressing when u hit pan on ur fa...      3
                                                text
0  “Worry is a down payment on a problem you may ...
1  My roommate: it's okay that we can't spell bec...
2  No but that's so cute. Atsu was probably shy a...
3  Rooneys fucking untouchable isn't he? Been fuc...
4  it's pretty depressing when u hit pan on ur fa...
   label
0      2
1      0
2      1
3      0
4      3
                                                text  label
0  @user @user Oh, hidden revenge and anger...I r...      0
1  if not then #teamchristine bc all tana has don...      0
2  Hey @user #Fields in #skibbereen give your onl...      0
3  Why have #Emmerdale had to rob #robron of havi...      0
4  @user I would like to hear a podcast of you go...      0
                                                text
0  @user @user Oh, hidden revenge and anger...I r...
1  if not then #teamchristine bc all tana has don...
2  Hey @user #Fields in #skibbereen give your onl...
3  Why have #Emmerdale had to rob #robron of havi...
4  @user I would like to hear a podcast of you go...
   label
0      0
1      0
2      0
3      0
4      0
                                                text  label
0  #Deppression is real. Partners w/ #depressed p...      3
1  @user Interesting choice of words... Are you c...      0
2  My visit to hospital for care triggered #traum...      3
3  @user Welcome to #MPSVT! We are delighted to h...      1
4                       What makes you feel #joyful?      1
                                                text
0  #Deppression is real. Partners w/ #depressed p...
1  @user Interesting choice of words... Are you c...
2  My visit to hospital for care triggered #traum...
3  @user Welcome to #MPSVT! We are delighted to h...
4                       What makes you feel #joyful?
                                                text
0  #Deppression is real. Partners w/ #depressed p...
1  @user Interesting choice of words... Are you c...
2  My visit to hospital for care triggered #traum...
3  @user Welcome to #MPSVT! We are delighted to h...
4                       What makes you feel #joyful?
len(train_labels) 3257
len(test_labels) 1421
len(val_labels) 374

Unique values count in train_labels:
label
0    1400
3     855
1     708
2     294
Name: count, dtype: int64

Unique values count in val_labels:
label
0    160
1     97
3     89
2     28
Name: count, dtype: int64

Unique values count in test_labels:
label
0    558
3    382
1    358
2    123
Name: count, dtype: int64





===================================================== 
flag 1.10  model:  started with ==>   bert
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.367
Validation Loss: 1.325
Validation Accuracy: 0.4305

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.312
Validation Loss: 1.276
Validation Accuracy: 0.4278


         == flag 1.601 bert result On test data ==
# called_model : bert
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   bert





===================================================== 
flag 1.10  model:  started with ==>   roberta
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.369
Validation Loss: 1.351
Validation Accuracy: 0.2380

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.294
Validation Loss: 1.240
Validation Accuracy: 0.4278


         == flag 1.601 roberta result On test data ==
# called_model : roberta
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   roberta





===================================================== 
flag 1.10  model:  started with ==>   distilbert
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.370
Validation Loss: 1.322
Validation Accuracy: 0.4251

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.282
Validation Loss: 1.245
Validation Accuracy: 0.4278


         == flag 1.601 distilbert result On test data ==
# called_model : distilbert
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   distilbert





===================================================== 
flag 1.10  model:  started with ==>   electra
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.357
Validation Loss: 1.313
Validation Accuracy: 0.4251

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.301
Validation Loss: 1.272
Validation Accuracy: 0.4278


         == flag 1.601 electra result On test data ==
# called_model : electra
# Test Accuracy: 0.3927%
Precision: 0.1542
Recall: 0.3927
F1 Score: 0.2214
Classification Report:
              precision    recall  f1-score   support

           0       0.39      1.00      0.56       558
           1       0.00      0.00      0.00       358
           2       0.00      0.00      0.00       123
           3       0.00      0.00      0.00       382

    accuracy                           0.39      1421
   macro avg       0.10      0.25      0.14      1421
weighted avg       0.15      0.39      0.22      1421

Confusion Matrix:
[[558   0   0   0]
 [358   0   0   0]
 [123   0   0   0]
 [382   0   0   0]]

flag 1.11  model:  finished  with:   electra





===================================================== 
flag 1.10  model:  started with ==>   gpt2
===================================================== 

 Epoch 1 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 2.222
Validation Loss: 1.766
Validation Accuracy: 0.2326

 Epoch 2 / 2
  Batch    50  of    102.
  Batch   100  of    102.

Evaluating...

Training Loss: 1.607
Validation Loss: 1.425
Validation Accuracy: 0.2968


         == flag 1.601 gpt2 result On test data ==
# called_model : gpt2
# Test Accuracy: 0.3202%
Precision: 0.3008
Recall: 0.3202
F1 Score: 0.2485
Classification Report:
              precision    recall  f1-score   support

           0       0.48      0.27      0.34       558
           1       0.14      0.00      0.01       358
           2       0.00      0.00      0.00       123
           3       0.28      0.80      0.42       382

    accuracy                           0.32      1421
   macro avg       0.23      0.27      0.19      1421
weighted avg       0.30      0.32      0.25      1421

Confusion Matrix:
[[149   1  13 395]
 [ 72   1   7 278]
 [ 22   1   0 100]
 [ 67   4   6 305]]

flag 1.11  model:  finished  with:   gpt2





===================================================== 
flag 1.10  model:  started with ==>   longformer
===================================================== 

 Epoch 1 / 2
